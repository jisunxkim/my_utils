{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15b0f90-d065-4ad4-87ec-51e968df8169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import argparse \n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import multiprocessing\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import ds_general as ds\n",
    "from utils.BigQuery import BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cbb6dcd-5f9e-4f33-a9a7-9edef9559e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_blob_list(config, train_or_test='train', verbose = 1):    \n",
    "    \"\"\"\n",
    "    return list of blobs of train or test data\n",
    "    \"\"\"\n",
    "    if train_or_test.lower() =='train':\n",
    "        blob = config['TRAIN_DATA_BLOB']\n",
    "        max_files = config['MAX_N_FILES']\n",
    "    elif train_or_test.lower() == 'test':\n",
    "        blob = config['TEST_DATA_BLOB']\n",
    "        max_files = config['TOTAL_FILES_TO_TEST']\n",
    "    else:\n",
    "        ds.terminate_prog(\"select 'train' or 'test' for data blob\", error = e)\n",
    "        \n",
    "    try: \n",
    "        blobs_list = bq.list_blobs(config['BUCKET'], f\"{blob}\")\n",
    "        blobs_list = [f\"gs://{config['BUCKET']}/\" + b for b in blobs_list]\n",
    "\n",
    "        n_blobs = len(blobs_list) if not max_files \\\n",
    "            else max_files\n",
    "        print(f\"total num of {train_or_test} blobs {len(blobs_list)}\")\n",
    "        print(f\"num of the selected {train_or_test} blobs {n_blobs}\")\n",
    "        return blobs_list[:n_blobs]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ds.terminate_prog(f\"Failed to load {train_or_test}blob list\", error = e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f1847f-e44b-47c2-b2c4-e8448811d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from blob\n",
    "def load_data_from_blob(blob_path):\n",
    "    \"\"\"\n",
    "    Load model data from a blob.\n",
    "    Return a data frame\n",
    "    \"\"\"\n",
    "    temp_df = pd.DataFrame()\n",
    "    result_df = pd.DataFrame()\n",
    "    try:\n",
    "        if type(blob_path) == str:\n",
    "            blob_path = [blob_path]\n",
    "        for i, blob in enumerate(blob_path):\n",
    "            print(f\"{i+1}th blob loading......\")\n",
    "            temp_df = pd.read_parquet(blob)\n",
    "            temp_df = temp_df.reset_index(drop=True)\n",
    "            result_df = result_df.reset_index(drop=True)\n",
    "            result_df = pd.concat([result_df, temp_df], axis=0)\n",
    "        \n",
    "        return result_df\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        msg = \"Failed to load the blob.\"\n",
    "        print(msg)\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg, error=e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef4d089-8b83-4bf5-ba93-8ea325e8a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_blob(\n",
    "    bucket, blob, \n",
    "    model_fname = \"./tmp/temp_pretrained_xgb_model\",\n",
    "    file_type = 'xgb',\n",
    "    verbose = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    file_type: \n",
    "        'xgb', load xgb model saved on blob by xgb_model.save().\n",
    "        'joblib', load a model saved by joblib\n",
    "        'pickle',  load a model saved by pickle\n",
    "    \"\"\"\n",
    "    if not blob:\n",
    "        print(\"No pretrained model is provided. Continue with a blank model.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        bq.download_file_from_blob(\n",
    "            bucket,\n",
    "            blob,\n",
    "            model_fname,\n",
    "            verbose\n",
    "        )\n",
    "        if file_type == 'xgb':\n",
    "            # model = xgb.Booster()\n",
    "            model = xgb.XGBClassifier()\n",
    "            model = model.load_model(model_fname)\n",
    "        elif file_type == 'joblib':\n",
    "            model = joblib.load(model_fname)\n",
    "        elif file_type == 'pickle':\n",
    "            with open(model_fname, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "\n",
    "        else:\n",
    "            logger.critical(\"file_type: wrong file_type. Should be either 'xgb', 'joblib', or 'pickle'\")\n",
    "            return\n",
    "        # logger.debug(\"Succefully loaded a pre-trained model. \")\n",
    "        logger.info(\"Succefully loaded a pre-trained model. \")\n",
    "\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        msg = f\"Failed to load pretrained xgb model from {blob} \\nContinue without a pre-trained model\"\n",
    "        print(msg)\n",
    "        print(e)\n",
    "        logger.critical(e)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3cba3-73b4-463d-be88-76dae5d69017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(df, config):\n",
    "    \"\"\"\n",
    "    feature engineering here\n",
    "    \"\"\"\n",
    "    print(\"*\"*5, \"feature engineering...\")\n",
    "    \n",
    "    # drop any rows with Null data\n",
    "    print('drop rows with any NA....')\n",
    "    print(f'df before drop na: {df.shape}')\n",
    "    df = df.dropna()\n",
    "    print(f'df afer drop na: {df.shape}')\n",
    "    \n",
    "    # drop a certain events\n",
    "    if config['DROP_EVENTS']:\n",
    "        exclude_events = config['DROP_EVENTS']\n",
    "        \n",
    "        print(f\"removing events: {exclude_events}\")\n",
    "        print(f'df before removing events: {df.shape}')\n",
    "        df = df[~df.event_id.isin(exclude_events)]\n",
    "        print(f'df after revmoved events: {df.shape}')\n",
    "        \n",
    "    if config['LABEL_ENCODER_COLS']:\n",
    "        for col in config['LABEL_ENCODER_COLS']:\n",
    "            label_encoder = LabelEncoder()\n",
    "            df[col] = label_encoder.fit_transform(df[col])\n",
    "        print(f\"Encorded string feature to numeric int features: {config['LABEL_ENCODER_COLS']}\")\n",
    "        \n",
    "    if config['CATEGORICAL_COLS']:\n",
    "        df[config['CATEGORICAL_COLS']] = df[config['CATEGORICAL_COLS']].astype(\"category\")\n",
    "        print(f\"Converted numeric features to categorical features: {config['CATEGORICAL_COLS']}\")\n",
    "    \n",
    "    print(\"*\"*5, \"feature engineering DONE\")\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ca6649-0093-4570-af78-353fefcb8a8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train_data(df, config, feature_modification, get_sample_weight = False):\n",
    "    \"\"\"\n",
    "    Convert raw dataframe to X, y train data.\n",
    "    Sample weight vectors of the length of the data is also returned if set True.\n",
    "    Include data cleaning.\n",
    "    \n",
    "    Return: (X, y, sample_weight_vector)\n",
    "        X: features panda dataframe (n_rows, n_cols)\n",
    "        y: panda series (n_rows, )\n",
    "        sample_weight_vector: numpy array (n_rows, )\n",
    "        \n",
    "    \"\"\"\n",
    "    result = ()\n",
    "    cols_to_drop = []\n",
    "    try:\n",
    "        \n",
    "         # light additional feature engineering\n",
    "        if feature_modification:\n",
    "            df = feature_eng(df, config)\n",
    "        \n",
    "        if get_sample_weight:\n",
    "            print('Applying multi label weights....')\n",
    "            print(config['LABEL_MULTI_WEIGHT'])\n",
    "            sample_weight_vector = df.label_multi.astype(int).map(config['LABEL_MULTI_WEIGHT'])\n",
    "        else:\n",
    "            sample_weight_vector = None\n",
    "            \n",
    "        for col in config['COLS_TO_DROP']:\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "        \n",
    "        print(f\"dropping columns before training: {cols_to_drop}\")\n",
    "        df = df.drop(cols_to_drop, axis = 1)\n",
    "    \n",
    "        y = df[config['Y_LABEL_COL']].astype(int)\n",
    "        X = df.drop(config['Y_LABEL_COL'], axis=1)\n",
    "               \n",
    "        result = (X, y, sample_weight_vector)\n",
    "        \n",
    "    except Exception as e:\n",
    "        msg = \"Failed to make train data from the dataframe!\"\n",
    "        print(msg)\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg, error = e)\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f01879-32c8-4d43-a23b-3553826a4c02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model_id(config, prefix='model_'):\n",
    "    if prefix:\n",
    "        return prefix + \"_\" + config['MODEL_VERSION'] + str(ds.name_time_id())\n",
    "    else:\n",
    "        return config['MODEL_VERSION'] + str(ds.name_time_id())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea5ebca5-01df-407b-98d5-18980f4b9cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config(f_path, verbose=2):\n",
    "    \"\"\"\n",
    "    Load a config yaml file.\n",
    "    \"\"\"\n",
    "    config = ds.load_config(f_path)\n",
    "    if verbose > 1:\n",
    "        print(config)\n",
    "    elif verbose > 0:\n",
    "        print(f\"config keys: {config.keys()}\")\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9b5772-1086-4ee8-af6f-9c73d8ed17d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_df_list(df_list):\n",
    "    if type(df_list) != list:\n",
    "        raise Exception(\"ERROR: should be a lsit of df to merge!!!\")\n",
    "    print(f\"merging {len(df_list)} dataframes started.....\")\n",
    "    try:\n",
    "        # result_df = pd.DataFrame()\n",
    "        # for df in df_list:\n",
    "        #     result_df = result_df.reset_index(drop=True)\n",
    "        #     result_df = pd.concat([result_df, df], axis=0)\n",
    "        \n",
    "        result_df = pd.concat(df_list)\n",
    "            \n",
    "            \n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        msg = \"failed to merge dataframes\"\n",
    "        print(msg)\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg=msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb1fa4a5-ffc2-46ec-bee2-2b4ab27e3769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load best parameters\n",
    "def load_best_params(bucket_name=None, blob_name=None, \n",
    "                     param_fname = None,\n",
    "                     param_dict = None,\n",
    "                     verbose = 1):\n",
    "    params = None\n",
    "    source = None\n",
    "    try:\n",
    "        if blob_name:\n",
    "            param_fname = './tmp/temp_best_params.pkl'\n",
    "            bq.download_file_from_blob(\n",
    "                bucket_name,\n",
    "                blob_name,\n",
    "                param_fname,\n",
    "                verbose\n",
    "            )\n",
    "            with open(param_fname, 'rb') as file:\n",
    "                params = pickle.load(file)\n",
    "                \n",
    "            source = \"BLOB\"\n",
    "        elif param_fname:\n",
    "            with open(param_fname, 'rb') as file:\n",
    "                params = pickle.load(file)\n",
    "                \n",
    "            source = \"FILE\"\n",
    "        elif param_dict:\n",
    "            params = param_dict\n",
    "            \n",
    "            source = \"CONFIG/MANUAL\"\n",
    "        else:\n",
    "            source = \"NO SOURCE\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        msg = \"Failed to load the best_params\"\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg=msg, error=e)\n",
    "    \n",
    "    print(f\"Hyper Parameters Loaded from {source}.....\")\n",
    "    if verbose > 0:\n",
    "        print(params)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d68f39-fb87-47bf-86cc-c2d89570e74a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_metrcs_inline(eval_y, y_pred, loss, best_iter):\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(eval_y, y_pred)\n",
    "    balanced_accuracy_value = balanced_accuracy_score(eval_y, y_pred)\n",
    "    f1_value = f1_score(eval_y, y_pred)\n",
    "    recall_val = recall_score(eval_y, y_pred)\n",
    "    roc_auc_val = roc_auc_score(eval_y, y_pred)\n",
    "    \n",
    "    min_loss = min(loss)\n",
    "    last_loss = loss[-1]\n",
    "    epochs = len(loss)\n",
    "\n",
    "    print(\n",
    "        \"|Accu: %.2f%%\" % (accuracy * 100.0), \\\n",
    "        \"|BalAccu: %.2f%%\" %(balanced_accuracy_value * 100.0), \\\n",
    "        \"|ROCAUC: %.3f\" %(roc_auc_val), \\\n",
    "        \"|RecallPos: %.3f\" %(recall_val), \\\n",
    "        \"|F1Pos: %.3f\" %(f1_value), \\\n",
    "        \"|Epochs: %.0i\" %(epochs), \\\n",
    "        \"|BestIterNum: %.0i\" %(best_iter), \\\n",
    "        \"|MinLoss: %.3f\" %(min_loss), \\\n",
    "        \"|LastLoss: %.3f\" %(last_loss) \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9faeb6e-4132-4a5d-8798-e7ae4e5af2fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgb model trainiing\n",
    "\n",
    "def xgb_train(\n",
    "    train_X, train_y, params,\n",
    "    eval_set_list=None,\n",
    "    sample_weight_vector = None,\n",
    "    trained_xbg_model = None,\n",
    "    verbose = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    parameters:\n",
    "        params: hyperparameter dictionary, None for default parameter value\n",
    "        objective_func: optimization objective function, 'binary:logistic', ''\n",
    "        tree_method: 'auto', 'approx', 'hist', 'gpu_hist'\n",
    "    Best model out of the validation: \n",
    "        When using cross-validation with XGBoost's fit method, \n",
    "        the model returned will be the one \n",
    "        that achieves the best performance on the validation set \n",
    "        during the cross-validation process. \n",
    "        It will not be the last model trained.\n",
    "        Using cross-validation with XGBoost helps\n",
    "        to improve the robustness of your model \n",
    "        by reducing overfitting and giving you an estimate \n",
    "        of the model's performance on unseen data.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        **params,\n",
    "        seed = 123)\n",
    "    \n",
    "    model.fit(\n",
    "        train_X, train_y,\n",
    "        eval_set=eval_set_list,\n",
    "        sample_weight = sample_weight_vector,\n",
    "        xgb_model=trained_xbg_model,\n",
    "        verbose = verbose\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0830eaf3-e6c8-4adc-be74-80f43b67d284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_data_multi_processing(blobs_list, config, feature_modification, get_sample_weight = False):\n",
    "    \"\"\"\n",
    "    Multi processing of loading model data including data cleaning.\n",
    "    Return: outputs of make_train_data()\n",
    "        \n",
    "    \"\"\"\n",
    "    try: \n",
    "        print(f\"Data loading total {len(blobs_list)} blobs starting {blobs_list[0]}...\")\n",
    "        \n",
    "        splited_loading_list = ds.split_list_n_size(\n",
    "                blobs_list, \n",
    "                config['N_FILES_EACH_LOADING'])\n",
    "        df_list = []\n",
    "        \n",
    "        p = multiprocessing.Pool(config['N_CPUS'])\n",
    "        pool_results = p.map(\n",
    "            func=load_data_from_blob, \n",
    "            iterable=splited_loading_list)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        \n",
    "        data_df = merge_df_list(pool_results)\n",
    "\n",
    "        results = make_train_data(\n",
    "            df=data_df,\n",
    "            config=config,\n",
    "            get_sample_weight=get_sample_weight, \n",
    "            feature_modification=feature_modification)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        msng = f\"Failed in geting model data by multi processing starting {blobs_list[0]}.\"\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg=msg, error=e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2ebee57-2f5d-4b07-a9f2-6eb911a9c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_file_name):\n",
    "    logger = ds.get_logger(level='INFO')\n",
    "    \n",
    "    ds.make_folder('tmp')\n",
    "    \n",
    "    main_time = ds.time_start_end(msg=\"MAIN\")\n",
    "    # load configuration from a yaml file\n",
    "    config = get_config(config_file_name)\n",
    "    \n",
    "    # model id and temp local model file name\n",
    "    # The model is saved in an XGBoost internal format \n",
    "    # which is universal among the various XGBoost interfaces. \n",
    "    # Auxiliary attributes of the Python Booster object (such as feature_names) \n",
    "    # will not be saved when using binary format. \n",
    "    # To save those attributes, use JSON/UBJ instead.\n",
    "    model_id = make_model_id(config, prefix=config[\"MODEL_ID_PREFIX\"])\n",
    "    temp_model_fname = f'./tmp/temp_training_model_save_{model_id}.json'\n",
    "    \n",
    "    # print note if exist\n",
    "    if config['NOTE']:\n",
    "        print(\"#\"*40)\n",
    "        print(f\"## NOTE: {config['NOTE']}\")\n",
    "        print(\"#\"*40)\n",
    "    \n",
    "    # load a base model\n",
    "    trained_model = load_model_from_blob(\n",
    "        bucket=config['BUCKET'], \n",
    "        blob=config['PREVIOUS_MODEL_BLOB'], \n",
    "        file_type=config['PREVIOUS_MODEL_TYPE'])\n",
    "    \n",
    "    # load parameters\n",
    "    params = load_best_params(\n",
    "        bucket_name=config['BUCKET'],\n",
    "        blob_name=config['HYPER_PARAMETERS_BLOB'],\n",
    "        param_dict=config['PARAMS']\n",
    "    )\n",
    "    \n",
    "    overriding_params = config['OVERRIDE_PARAMS']\n",
    "    \n",
    "    is_overriding_params = False\n",
    "    \n",
    "    if overriding_params:\n",
    "        for param_name, param_value in overriding_params.items():\n",
    "            params[param_name] = param_value\n",
    "            is_overriding_params = True\n",
    "            \n",
    "    print(f\"overriding parameters: {is_overriding_params}\")\n",
    "    print(f\"Final training parameters: {params}\")\n",
    "    \n",
    "    # load list of blobs\n",
    "    # actual train test will be loaded in each batch process\n",
    "    blobs_list = load_blob_list(config, train_or_test='train')\n",
    "    \n",
    "    #############################\n",
    "    # load test blobs and data\n",
    "    #############################\n",
    "    test_data_time = ds.time_start_end(msg=\"test_data_load\")\n",
    "    test_blobs_list = load_blob_list(config, train_or_test='test')\n",
    "    test_X, test_y, test_sample_weight_vector = get_model_data_multi_processing(\n",
    "        test_blobs_list, config, \n",
    "        get_sample_weight=False, \n",
    "        feature_modification = config['FEATURE_MODIFICATION'])\n",
    "    \n",
    "    ds.df_info(test_X, label=\"test data feature dataframe\")\n",
    "    ds.time_start_end(started=test_data_time, msg=\"test_data_load\")\n",
    "    \n",
    "    \n",
    "    #############################\n",
    "    # Train in Bath processing in Serial: \n",
    "    # continuous Training batch by batch\n",
    "    #############################\n",
    "    split_batch_blob_list = ds.split_list_n_size(\n",
    "        blobs_list, \n",
    "        config['N_FILES_IN_BATCH'])\n",
    "    total_batch = len(split_batch_blob_list)\n",
    "    \n",
    "    # Iterate each batch\n",
    "    for i, blobs_list in enumerate(split_batch_blob_list):\n",
    "        n_batch = i + 1\n",
    "        batch_idx =f\"Training Batch {str(n_batch)}/{str(total_batch)}\"\n",
    "        print(f\"**** Starting Batch Training: {batch_idx} ****\")\n",
    "        #############################\n",
    "        # load train data from the blobs \n",
    "        #############################\n",
    "        batch_time = ds.time_start_end(\n",
    "            msg=f\"{batch_idx} train data loading from blobs\")\n",
    "            # multi processing - loading from blobs: \n",
    "        X, y, sample_weight_vector = get_model_data_multi_processing(\n",
    "            blobs_list, config, \n",
    "            get_sample_weight=config['USE_SAMPLE_WEIGHT'], \n",
    "            feature_modification = config['FEATURE_MODIFICATION'])\n",
    "            \n",
    "        ############\n",
    "        ds.df_info(X, label=f\"{batch_idx} feature dataframe\")\n",
    "        ds.time_start_end(started=batch_time, msg=f\"{batch_idx} train data loading from blobs\")\n",
    "    \n",
    "        #############################\n",
    "        # train on the data\n",
    "        #############################\n",
    "        batch_train_time = ds.time_start_end(msg=f\"{batch_idx} Training\")\n",
    "\n",
    "        # eval_set = ([(X, y), (test_X, test_y)])\n",
    "        # test dat as eval_set\n",
    "        eval_set = ([(test_X, test_y), (X, y)])\n",
    "        \n",
    "        try: \n",
    "            model = xgb_train(\n",
    "                train_X = X, \n",
    "                train_y = y, \n",
    "                eval_set_list = eval_set, \n",
    "                params = params,\n",
    "                sample_weight_vector = sample_weight_vector,\n",
    "                trained_xbg_model = trained_model,\n",
    "                verbose = config['N_VALID_OUTPUT'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg = \"Failed in training\"\n",
    "            print(msg, e)\n",
    "            ds.terminate_prog(msg=msg, error=e)\n",
    "        \n",
    "        #############################\n",
    "        # Get the evaluation results of the batch\n",
    "        #############################\n",
    "        eval_results = model.evals_result()\n",
    "        loss_values = eval_results['validation_1']['logloss']  # Change 'rmse' to the appropriate metric used during training\n",
    "        best_iter = model.best_iteration\n",
    "        y_pred = model.predict(test_X)\n",
    "        print_metrcs_inline(test_y, y_pred, loss_values, best_iter)\n",
    "        \n",
    "        #############################\n",
    "        # update trained model with the current batch trained model\n",
    "        #############################\n",
    "        try:\n",
    "            # trained_model = xgb.Booster()\n",
    "            trained_model = xgb.XGBClassifier()\n",
    "            model.save_model(temp_model_fname)\n",
    "            trained_model.load_model(temp_model_fname)\n",
    "            print(f\"Saved the trained model of the batch: {temp_model_fname}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg= f\"Failed to save the trained model of {batch_idx}.\"\n",
    "            print(msg, e)\n",
    "            ds.terminate_prog(msg, e)\n",
    "            \n",
    "        ds.time_start_end(started=batch_train_time, msg=f\"{batch_idx} Training\")\n",
    "        ds.time_start_end(started=batch_time, msg=f\"{batch_idx}\")\n",
    "    \n",
    "    \n",
    "    ds.time_start_end(started=main_time, msg=\"MAIN\")\n",
    "    \n",
    "    #############################\n",
    "    # Save best_model and upload to blob\n",
    "    #############################\n",
    "    file_name = 'trained_model_' + model_id +'.pickle'\n",
    "    blob_name = config['ARTIFACT_BLOB'] + '/' + file_name\n",
    "\n",
    "    with open(file_name, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    \n",
    "    if config['UPLOAD_MODEL']:\n",
    "        print(f\"Uploading the trained model, {file_name} to GCS\") \n",
    "        bq.upload_file_to_blob(file_name, config['BUCKET'], blob_name)\n",
    "        print(f\"uploaded the trained model to {blob_name}\")\n",
    "    else:\n",
    "        print(\"NOT uploaded to BLOB. Set True in config to upload!\")\n",
    "              \n",
    "    ## Remove the local model file\n",
    "    if config['REMOVE_LOCAL_TEMP_MODEL']:\n",
    "        os.remove(file_name)\n",
    "        print(\"Removed local model file\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6fb64-8e5b-4e3e-8242-330322f3b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config_file_name\", help=\"config file name\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    bq = BigQuery()\n",
    "    \n",
    "    main(args.config_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24503cd4-96f5-485c-8beb-47873b59b36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
