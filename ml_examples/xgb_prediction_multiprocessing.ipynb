{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6657c1f-b52a-447f-9cf0-f55c7a8d8a17",
   "metadata": {
    "tags": []
   },
   "source": [
    "# XGB Prediction - Multi Processing and Multi Threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1433afa-b02d-4d97-807e-afa304910f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "from utils import ds_general as ds\n",
    "from utils.BigQuery import BigQuery\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414020db-d139-45ea-83f4-9754095afc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blob_list(config, verbose = 1):    \n",
    "    \"\"\"\n",
    "    return list of blobs of train or test data\n",
    "    \"\"\"\n",
    "    \n",
    "    try: \n",
    "        max_files = config['TOTAL_FILES_TO_PREDICT']\n",
    "        blobs_list = bq.list_blobs(config['BUCKET'], config['FEATURE_DATA_BLOB'])\n",
    "        blobs_list = [f\"gs://{config['BUCKET']}/\" + b for b in blobs_list]\n",
    "\n",
    "        n_blobs = len(blobs_list) if not max_files \\\n",
    "            else max_files\n",
    "        print(f\"total num of blobs {len(blobs_list)}\")\n",
    "        print(f\"num of the selected blobs {n_blobs}\")\n",
    "        return blobs_list[:n_blobs]\n",
    "    \n",
    "    except Exception as e:\n",
    "        ds.terminate_prog(f\"Failed to load {train_or_test}blob list\", error = e)\n",
    "\n",
    "# load data from blob\n",
    "def load_data_from_blob_thread(blob_path):\n",
    "    \"\"\"\n",
    "    Load model data from a blob.\n",
    "    Return a data frame\n",
    "    \"\"\"\n",
    "    t_name = thread_name = threading.current_thread().name\n",
    "    logger = ds.get_logger(name=t_name, level=\"INFO\")\n",
    "    # Perform some computation and return a result\n",
    "    print(f\"Thread name: {t_name}\")\n",
    "    temp_df = pd.DataFrame()\n",
    "    result_list = []\n",
    "    result_df = pd.DataFrame()\n",
    "    try:\n",
    "        if not isinstance(blob_path, list):\n",
    "            blob_path = [blob_path]\n",
    "            \n",
    "        for i, blob in enumerate(blob_path):\n",
    "            temp_df = pd.read_parquet(blob)\n",
    "            result_list.append(temp_df)\n",
    "            logger.debug(f\"{i+1}th blob loading: {blob}, shape:{temp_df.shape}\")\n",
    "            \n",
    "        result_df = pd.concat(result_list, axis=0)\n",
    "        logger.debug(ds.df_info(result_df))\n",
    "        logger.debug(result_df.iloc[:2, :5])\n",
    "        return result_df\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        msg = \"Failed to load the blob.\"\n",
    "        ds.terminate_prog(msg, error=e)\n",
    "\n",
    "def feature_eng(df, config):\n",
    "    cols = df.columns\n",
    "    cols_to_drop = [col for col in config['COLS_TO_DROP'] if col in cols]\n",
    "    \n",
    "    idx = df[config['IDX_COLS']].copy()\n",
    "    df = df.drop(cols_to_drop, axis = 1)\n",
    "    \n",
    "    return df, idx\n",
    "\n",
    "\n",
    "def load_xgb_model_from_file(model_fname, file_type):\n",
    "    process_name = multiprocessing.current_process().name\n",
    "    logger = ds.get_logger(name=process_name, level=\"INFO\")\n",
    "    \n",
    "    try:\n",
    "        if file_type == 'xgb':\n",
    "            print(\"loading model by xgboost\")\n",
    "            # model = xgb.Booster()\n",
    "            model = xgb.XGBClassifier()\n",
    "            model = model.load_model(model_fname)\n",
    "            print(f\"check loaded mode: {model.best_score}\")\n",
    "        elif file_type == 'joblib':\n",
    "            print(\"loading model by joblib\")\n",
    "            model = joblib.load(model_fname)\n",
    "        elif file_type == 'pickle':\n",
    "            print(\"loading model by pickle\")\n",
    "            with open(model_fname, 'rb') as f:\n",
    "                model = pickle.load(f)\n",
    "        else:\n",
    "            logger.critical(\"file_type: wrong file_type. Should be either 'xgb', 'joblib', or 'pickle'\")\n",
    "            return\n",
    "\n",
    "        if not model:\n",
    "            ds.terminate_prog(msg=\"Model is None!!! check the local model file\")\n",
    "        else:\n",
    "            print(f\"Loaded model from disk; {model}\")\n",
    "            logger.debug(\"Succefully loaded a pre-trained model. \")\n",
    "\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to load pretrained xgb model from {blob} \\nContinue without a pre-trained model\")\n",
    "        logger.critical(e)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21208db9-52b1-4e65-8208-557862f29242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(data_blob_list, config):\n",
    "    \"\"\"\n",
    "    load feature data of the parquet files from the blob list.\n",
    "    load config.\n",
    "    load a model.\n",
    "    predict and append to the bq table.\n",
    "    \"\"\"\n",
    "    process_name = multiprocessing.current_process().name\n",
    "    logger = ds.get_logger(name=process_name, level=\"INFO\")\n",
    "    \n",
    "    bq = BigQuery(project_id=config[\"GCP_PROJECT\"])\n",
    "    \n",
    "    first_blob = data_blob_list[0].split(\"/\")[-1]\n",
    "    logger.info(f\"*** PREDICTION, begining blob: {first_blob}\")\n",
    "\n",
    "    \n",
    "    # load trained model\n",
    "    if config['MODEL_LOCAL_FILE_PATH']:\n",
    "        model_path = config['MODEL_LOCAL_FILE_PATH']\n",
    "    else:\n",
    "        model_path = config['TEMP_MODEL_FILE_NAME']\n",
    "\n",
    "    print(f\"loading model from {model_path}\")\n",
    "    \n",
    "    model = load_xgb_model_from_file(\n",
    "        model_fname=model_path,\n",
    "        file_type = config['MODEL_TYPE'])\n",
    "\n",
    "    # TREAD SUB BATCH: split the blobs for multi processing\n",
    "    splited_blobs_to_read = ds.split_list_n_size(\n",
    "        data_blob_list, config['N_THREADS'])\n",
    "    \n",
    "    # load data from the blob list by multi processing\n",
    "    data_df = pd.DataFrame()\n",
    "    \n",
    "    # Create a ThreadPoolExecutor with the desired number of threads\n",
    "    threads_results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=config['N_THREADS']) as executor:\n",
    "        # Submit each thread job and get a Future object\n",
    "        futures = [\n",
    "            executor.submit(load_data_from_blob_thread, blob_list) \\\n",
    "            for blob_list in splited_blobs_to_read\n",
    "        ]\n",
    "\n",
    "        # Wait for all threads to complete and get results\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            threads_results.append(result)\n",
    "\n",
    "    # Now, 'threads_results' list contains the return dataframes from each thread\n",
    "    data_df = pd.concat(threads_results, axis=0)\n",
    "    logger.debug(ds.df_info(data_df))\n",
    "    \n",
    "    # feature engineering, and generate feature data and index\n",
    "    X, idx_df = feature_eng(data_df, config)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        pred_y = pd.DataFrame(data=model.predict_proba(X), columns=['neg_score', 'pos_score'])\n",
    "        pred_y = pred_y.reset_index(drop=True)\n",
    "        idx_df = idx_df.reset_index(drop=True)\n",
    "        pred_y = pd.concat([idx_df, pred_y], axis=1)\n",
    "        logger.info(\"Prediction of the batch done.\")\n",
    "        \n",
    "        logger.debug(\"\\n\" + str(pred_y.head(2)))\n",
    "        logger.info(pred_y.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if config['WRITE_TABLE']:\n",
    "            # write prediction to a bq table.\n",
    "            # delay in order to avoid rateLimitExceeded in updating bq table.\n",
    "            time.sleep(random.randrange(2,4))\n",
    "\n",
    "            bq.df_to_table(\n",
    "                dataframe = pred_y, \n",
    "                destination=config['PREDICTION_RESULT_TABLE'], \n",
    "                write_method='WRITE_APPEND')\n",
    "        \n",
    "            logger.debug(f\"Appended the prediction to a bq table: {config['PREDICTION_RESULT_TABLE']}\")\n",
    "        \n",
    "            # delay in order to avoid rateLimitExceeded in updating bq table.\n",
    "            time.sleep(random.randrange(2,4))\n",
    "        \n",
    "    except Exception as e:\n",
    "        msg=f\"Failed to predict, begining blob: {data_blob_list[0]}\"\n",
    "        logger.critical(e)\n",
    "        ds.terminate_prog(msg=msg,error=e)\n",
    "    \n",
    "\n",
    "    return pred_y.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de5698-50ae-4454-aaa0-aea1566a2cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    \n",
    "    process_name = multiprocessing.current_process().name\n",
    "    logger = ds.get_logger(name=process_name, level=\"INFO\")\n",
    "    \n",
    "    main_time = ds.time_start_end(msg=\"MAIN\")\n",
    "    \n",
    "    # Load blob list\n",
    "    logger.info(\"Loading prediction blobs...\")\n",
    "    data_blob_list = load_blob_list(config)\n",
    "\n",
    "    if not config['MODEL_LOCAL_FILE_PATH']:\n",
    "        # Download a model from a blob\n",
    "        logger.info(\"Downloading model from a blob...\")\n",
    "        try:\n",
    "            bq.download_file_from_blob(\n",
    "                bucket_name=config['BUCKET'], \n",
    "                source_blob_name=config['MODEL_BLOB'],\n",
    "                destination_file_name=config['TEMP_MODEL_FILE_NAME'],\n",
    "                verbose=1)\n",
    "        except Exception as e:\n",
    "                msg=f\"Failed to load pretrained xgb model from {config['MODEL_BLOB']} \\nContinue without a pre-trained model\"\n",
    "                logger.critical(e)\n",
    "                ds.terminate_prog(msg, error=e)\n",
    "    \n",
    "    # BATCH: split the blob list for multiprocessing\n",
    "    splited_data_blob_list = ds.split_list_n_size(\n",
    "        data_blob_list,\n",
    "        config['N_FILES_IN_BATCH'])\n",
    "    \n",
    "    # arguments tuple for multi processing: [(blob_list, config)]\n",
    "    tuple_args_list = [(b_list, config) for b_list in splited_data_blob_list]\n",
    "    \n",
    "    # run multiprocessing\n",
    "    p = multiprocessing.Pool(config['N_CPUS'])\n",
    "    results = p.starmap(predict_batch, tuple_args_list)\n",
    "    \n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "    print(\"DONE ALL multiprocessing and threading.\")\n",
    "    logger.info(\"Number of rows predicted each batch:\" + str(results))\n",
    "    total_rows = 0 \n",
    "    for n_rows in results:\n",
    "        total_rows += n_rows if n_rows else 0\n",
    "    logger.info(\"Total number of rows predicted:\" + str(total_rows))\n",
    "    \n",
    "    if config['WRITE_TABLE']:\n",
    "        print(f\"Prediction bq table: {config['PREDICTION_RESULT_TABLE']}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e4a18-44e1-43fd-b725-d46e0b8197a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('config_path', help='config file path and name')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # load config file\n",
    "    config = ds.load_config(args.config_path)\n",
    "    \n",
    "    ds.make_folder('tmp')\n",
    "    \n",
    "    # load bigquery client\n",
    "    bq = BigQuery(project_id=config[\"GCP_PROJECT\"])\n",
    "    \n",
    "\n",
    "    main(config)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757015d-2f26-4338-abbc-155b14fd4702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_predicrt.sh\n",
    "# # !/bin/bash\n",
    "# jupyter nbconvert xgb_prediction_multiprocessing.ipynb --to python \n",
    "# nohup python3 -u xgb_prediction_multiprocessing.py ./config/./config/xgb_prediction_config01.yaml.yaml > model_07282023_210857_prediction_xgb_fv2_2_testset_jan_2023_08022023.log 2>1&"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kf_base",
   "language": "python",
   "name": "kf_base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
