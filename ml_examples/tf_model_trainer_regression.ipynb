{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Model training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and other initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from kubeflow import fairing \n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import imp\n",
    "import yaml\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve\n",
    "from sklearn.preprocessing import normalize\n",
    "from google.cloud import storage\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.append('../utilities/')\n",
    "import modeldb_tf_utilities\n",
    "import evaluation_utilities\n",
    "imp.reload(modeldb_tf_utilities)\n",
    "imp.reload(evaluation_utilities)\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmenon\n",
      "zulilymodeltraining\n"
     ]
    }
   ],
   "source": [
    "# fairing:include-cell\n",
    "# Global configs that would be used at all the steps of the notebook.\n",
    "GCP_PROJECT = fairing.backends.gcp.guess_project_name()\n",
    "NAMESPACE = fairing.backends.utils.get_current_k8s_namespace()\n",
    "PROJECT_ID = GCP_PROJECT\n",
    "MODELDB_CLIENT_URL = \"https://modeldb.mlp.ml.gce.z8s.io/\"\n",
    "print(NAMESPACE)\n",
    "print(GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model and Data Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "# YAML file containing model data configuration: i.e. Feature names, identification of categorical names etc\n",
    "model_data_config_file_name = \"./../model_configs/text_based_config.yaml\"\n",
    "\n",
    "# Dataset paths\n",
    "data_config = {\n",
    "    #\"training\": 'gs://personalization-tensorflow/data/train/all_features_tf_classification_v07_train_mobile_imp_04_58*.csv', \n",
    "    #\"training\": 'gs://personalization-tensorflow/data/train/all_features_tf_classification_v07_train_mobile_imp_0_458*.csv', \n",
    "    #\"validation\": 'gs://personalization-tensorflow/data/valid/all_features_tf_classification_v07_valid_diff*.csv',\n",
    "    #\"test\": 'gs://personalization-tensorflow/data/test/all_features_tf_classification_v07_test_diff*.csv',\n",
    "    \"training\":'gs://zulilymodeltraining/rmenon/data/train/all_features_text_tf_classification_v07_train_mobile_imp_0_458*.csv',\n",
    "    \"validation\":'gs://zulilymodeltraining/rmenon/data/valid/all_features_text_tf_classification_v07_valid_diff*.csv',\n",
    "    #\"test\":'gs://zulilymodeltraining/rmenon/data/test/all_features_text_tf_classification_v07_test_diff*.csv',    \n",
    "    \"test\":'gs://zulilymodeltraining/rmenon/data/test/all_features_text_tf_classification_v07_test_test_v2*.csv',\n",
    "}\n",
    "\n",
    "# Path to save trained model and other model-related specs\n",
    "model_data_path_prefix= f\"gs://personalization-tensorflow/models/text_features/\"\n",
    "\n",
    "# Model training parameters\n",
    "model_fit_config = {\n",
    "    \"batch_size\": 2048,\n",
    "    \"initial_lr\": 1e-3,\n",
    "    \"epochs\": 50\n",
    "}\n",
    "\n",
    "# Evaluating LTR performance metrics\n",
    "max_rank = 15\n",
    "file_path_to_bs_results = \"bs_results_on_test_test_v2.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalizer Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "# Parameters related to feature normalizer for the model\n",
    "num_samples_to_train_normalizer = None # Set a sample size (in terms of number of batches). If set to None, the entire \"training\" set will be used to train the normalizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model DB Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "# Model DB configuration parameters\n",
    "modeldb_config = {\n",
    "    ## Required configs\n",
    "    # These are required configs for a modeldb run. \n",
    "    # Please refer to notes here: https://confluence.zulily.com/display/tech/Notes+about+using+ModelDB if you are updaing the default\n",
    "    # project and experiment name.\n",
    "    \"client_url\": MODELDB_CLIENT_URL,\n",
    "    \"project_name\": 'P13N_Event_Sort_Models_2021',\n",
    "    \"experiment_name\": f\"text-features\",\n",
    "    # Username is mapped into as a ModelDB tag which will help to identiy a run by an user.\n",
    "    \"username\": NAMESPACE,\n",
    "    \n",
    "    ## Optional configs\n",
    "    # If an experiment run name is not specified, then ModelDB will randomly assign a run_name.\n",
    "    \"experiment_run_name\": 'text_features_4layer_1024_target_0_458_regression',\n",
    "    # This parameter is by default true and is required if you are going to run multiple runs with same experiment_run_name.\n",
    "    # This will prevent you from overwritng an experiment_run data and create a new run everytime a pipeline runs.\n",
    "    \"add_random_hash_to_run_name\": 'true',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Internal initializations based on YAML configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "# Retrieve model configuration from YAML file.\n",
    "with open(model_data_config_file_name) as file:\n",
    "    model_data_config = yaml.load(file, Loader=yaml.SafeLoader)\n",
    "# Do some name mappings to make code cleaner\n",
    "feature_names = model_data_config['feature_names']\n",
    "categorical_columns = model_data_config['categorical_columns']\n",
    "categorical_columns_vocabulary_list = model_data_config['categorical_columns_vocabulary_list']\n",
    "numeric_columns_to_norm = model_data_config['numeric_columns_to_norm']\n",
    "numeric_columns_remaining = [xx for xx in feature_names if ((xx not in categorical_columns) \\\n",
    "                                                            and (xx not in numeric_columns_to_norm))]\n",
    "target_name = model_data_config['target_name']\n",
    "numeric_columns_remaining.remove(target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create/ Load a Feature Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "def google_file_path_exists(path_name):\n",
    "    \"\"\"\n",
    "    Checks if a file path exists in google storage\n",
    "    path_name should be something like 'gs://zulilymodeltraining/rmenon/tf-models-data/normalizer_models/saved_model.pb'\n",
    "    \"\"\"    \n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    interim = path_name.split('//') # Extract gs:\n",
    "    interim = interim[1].split('/') # Extract stuff after gs, 'zulilymodeltraining/rmenon/tf-models-data/normalizer_models/saved_model.pb'\n",
    "    bucket_name = interim[0] # Extract bucket name, 'zulilymodeltraining'\n",
    "    name = ('/').join(interim[1:]) #Create file path excluding bucket name, 'rmenon/tf-models-data/normalizer_models/saved_model.pb'\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    stats = storage.Blob(bucket=bucket, name=name).exists(storage_client)\n",
    "    return(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizer training took 104.08817338943481secs\n"
     ]
    }
   ],
   "source": [
    "# fairing:include-cell\n",
    "st = time.time()\n",
    "\n",
    "# Create a data generator to run thru the training data\n",
    "column_defaults = ['float32' for column in numeric_columns_to_norm]\n",
    "data_batches_for_norm = tf.data.experimental.make_csv_dataset(\n",
    "    file_pattern = data_config[\"training\"], \n",
    "    select_columns = numeric_columns_to_norm,\n",
    "    column_defaults = column_defaults,\n",
    "    num_epochs=1, # Only want to go thru this data once in 1 training epoch    \n",
    "    ignore_errors = True,\n",
    "    batch_size = 2048    \n",
    "    )\n",
    "\n",
    "# Stack features: Change from dictionary format to a a stacked tensor array\n",
    "def stack_features(features):\n",
    "    return tf.stack(list(features.values()), axis=1)\n",
    "data_batches_for_norm_stacked = data_batches_for_norm.map(stack_features)\n",
    "\n",
    "# Pick a random sample if specified\n",
    "if num_samples_to_train_normalizer is not None:\n",
    "    data_batches_for_norm_stacked = data_batches_for_norm_stacked.take(int(num_samples_to_train_normalizer))\n",
    "\n",
    "# Train the normalizer \n",
    "feature_normalizer = preprocessing.Normalization()\n",
    "feature_normalizer.adapt(data_batches_for_norm_stacked)\n",
    "print('Normalizer training took {}secs'.format(time.time() - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "# Model-DB logging functions\n",
    "def log_model_attributes(modeldb_expt_run):\n",
    "    \"\"\"\n",
    "    Capturing Model attributes before starting training in ModelDB.\n",
    "    \"\"\"\n",
    "    modeldb_expt_run.log_hyperparameters(model_fit_config)\n",
    "    modeldb_expt_run.log_attributes(data_config)\n",
    "    modeldb_expt_run.log_attributes(model_data_config)\n",
    "\n",
    "    \n",
    "def log_model_metrics(modeldb_expt_run, model, model_save_path, test_ds = None):\n",
    "    \"\"\"\n",
    "    Capturing Model metrics at the end of training in ModelDB.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log the paths where the model and related data were saved\n",
    "    modeldb_expt_run.log_artifact_path('other_model_related_data_path', model_data_path_prefix)\n",
    "    modeldb_expt_run.log_artifact_path('model_save_path', model_save_path)\n",
    "    \n",
    "    # Log accuracy of the supplied data set (if supplied)\n",
    "    if test_ds is not None:\n",
    "#         loss, accuracy, precision, recall = model.evaluate(test_ds)        \n",
    "#         modeldb_expt_run.log_metric('loss', loss)\n",
    "#         modeldb_expt_run.log_metric('accuracy', accuracy)\n",
    "#         modeldb_expt_run.log_metric('precision', precision)\n",
    "#         modeldb_expt_run.log_metric('recall', recall)\n",
    "        loss = model.evaluate(test_ds)        \n",
    "        modeldb_expt_run.log_metric('loss', loss)\n",
    "        \n",
    "\n",
    "def log_model_summary(modeldb_expt_run, model):\n",
    "    \"\"\"\n",
    "    Log the structure of the Model\n",
    "    \"\"\"\n",
    "    stringlist = []\n",
    "    # Only store the last sequential layer\n",
    "    model.get_layer(index=-1).summary(print_fn=lambda x: stringlist.append(x))\n",
    "    short_model_summary = \"\\n\".join(stringlist)    \n",
    "    \n",
    "    if os.path.exists('/tmp/model/'):        \n",
    "        shutil.rmtree('/tmp/model')\n",
    "    os.mkdir('/tmp/model')\n",
    "\n",
    "    with open('/tmp/model/model.txt', 'w') as f:\n",
    "        f.write(short_model_summary)\n",
    "    f.close()\n",
    "    modeldb_expt_run.log_artifact('Model_Summary', '/tmp/model/model.txt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "# Target variable mapping function\n",
    "def parse_label_from_data(features, labels):\n",
    "    \"\"\"\n",
    "    Function to map the data parsed in order to generate the labels\n",
    "    \"\"\"\n",
    "   \n",
    "    label_0_values = tf.constant([0], dtype=tf.dtypes.int32)    \n",
    "    labels = tf.reshape(labels, [-1, 1])\n",
    "    labels_converted = tf.where(tf.reduce_any(tf.equal(labels, label_0_values), axis=1), \n",
    "                              tf.constant(0, dtype=tf.dtypes.int64), \n",
    "                              tf.constant(1, dtype=tf.dtypes.int64)) \n",
    "    return features, labels_converted\n",
    "\n",
    "# CSV Data generator\n",
    "def get_dataset_generator(file_path, target_name, feature_names, shuffle_dataset = True):\n",
    "    \"\"\"\n",
    "    Dataset does not need to be shuffled for validation and testing\n",
    "    \"\"\"\n",
    "    data_batches = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern = file_path, \n",
    "        select_columns = feature_names,\n",
    "        num_epochs=1, # Only want to go thru this data once in 1 training epoch\n",
    "        label_name=target_name,\n",
    "        ignore_errors = True,\n",
    "        shuffle = shuffle_dataset,\n",
    "        batch_size = model_fit_config['batch_size'],\n",
    "        sloppy = True, # Better reading performance since data reads will not be deterministic\n",
    "        prefetch_buffer_size = 1,\n",
    "        num_parallel_reads = 3 # Set this to >1 only if using multiple CPUs\n",
    "        )\n",
    "    #data_batches = data_batches.map(parse_label_from_data, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    return data_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "def get_sort_model(numeric_preprocessor):\n",
    "    \"\"\"\n",
    "    preprocessor: Any tensorflow preprocessing modules\n",
    "    inputs: Inputs to the tensorflow model - will determine the size of the input layer\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create input definitions for the model\n",
    "    inputs = {}\n",
    "    numeric_norm_inputs = {}\n",
    "    for header in numeric_columns_to_norm:            \n",
    "        numeric_norm_inputs[header] = tf.keras.Input(shape=(1,), name=header, dtype=tf.float32)         \n",
    "        inputs[header] = numeric_norm_inputs[header]\n",
    "    \n",
    "    remaining_inputs = {}\n",
    "    for header in numeric_columns_remaining:            \n",
    "        remaining_inputs[header] = tf.keras.Input(shape=(1,), name=header, dtype=tf.float32)         \n",
    "        inputs[header] = remaining_inputs[header]\n",
    "        \n",
    "    for header in categorical_columns:            \n",
    "        remaining_inputs[header] = tf.keras.Input(shape=(1,), name=header, dtype=tf.int64)         \n",
    "        inputs[header] = remaining_inputs[header]\n",
    "    \n",
    "    # Use the normalizer for features to be normalized\n",
    "    numeric_norm_inputs = layers.Concatenate()(list(numeric_norm_inputs.values()))\n",
    "    numeric_norm_preprocessed_inputs = numeric_preprocessor(numeric_norm_inputs)\n",
    "    \n",
    "    #Set up feature columns for other features\n",
    "    feature_columns = []\n",
    "    # numeric cols\n",
    "    for column in numeric_columns_remaining:\n",
    "        feature_columns.append(tf.feature_column.numeric_column(column))\n",
    "        \n",
    "    # Create categorical feature preprocessor    \n",
    "    for column in categorical_columns:\n",
    "        categorical_feature = tf.feature_column.categorical_column_with_vocabulary_list(column, \\\n",
    "                                                                                        categorical_columns_vocabulary_list[column],\\\n",
    "                                                                                        default_value = -1,\\\n",
    "                                                                                       dtype=tf.dtypes.int64)\n",
    "        categorical_feature_one_hot = tf.feature_column.indicator_column(categorical_feature)\n",
    "        feature_columns.append(categorical_feature_one_hot) \n",
    "    \n",
    "    # Define preprocessing layer\n",
    "    pre_processing_layer = tf.keras.layers.DenseFeatures(feature_columns=feature_columns)\n",
    "    preprocessed_inputs = pre_processing_layer(remaining_inputs)\n",
    "    \n",
    "    # Put together categorical and numerical features\n",
    "    preprocessed_inputs = layers.Concatenate()([numeric_norm_preprocessed_inputs, preprocessed_inputs])\n",
    "    \n",
    "    # Define the inner trainable layers of the sort model\n",
    "    sort_model_body = tf.keras.Sequential([\n",
    "            layers.Dense(1024, activation='relu'),    \n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dense(128, activation='relu'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1), # activation = 'sigmoid'\n",
    "        ])\n",
    "    \n",
    "    # Define flow thru inputs to the results stage. All done with stand-in for inputs \n",
    "    result = sort_model_body(preprocessed_inputs)\n",
    "    \n",
    "    # Put together the model\n",
    "    sort_model = tf.keras.Model(inputs, result)\n",
    "    \n",
    "    return sort_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "# Get test and validation data generators\n",
    "training_data = get_dataset_generator(data_config['training'], target_name, feature_names)\n",
    "validation_data = get_dataset_generator(data_config['validation'], target_name, feature_names, shuffle_dataset=False)\n",
    "test_features = list(feature_names)\n",
    "test_features.append('customer_id') # Need this information to evaluate against test dataset for LTR metrics\n",
    "test_data = get_dataset_generator(data_config['test'], target_name, feature_names, shuffle_dataset=False)\n",
    "# training_data = training_data.take(1)\n",
    "# validation_data = validation_data.take(1)\n",
    "# test_data = test_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successfully established\n",
      "got existing Project: P13N_Event_Sort_Models_2021\n",
      "got existing Experiment: text-features\n",
      "created new ExperimentRun: text_features_4layer_1024_target_0_458_regression_bhcb1jke\n",
      "upload complete (Model_Summary)\n"
     ]
    }
   ],
   "source": [
    "# fairing:include-cell\n",
    "initial_lr = model_fit_config['initial_lr']\n",
    "num_epochs = model_fit_config['epochs']\n",
    "\n",
    "# Create Model-DB Instance\n",
    "modeldb_expt_run = modeldb_tf_utilities.create_modeldb_experiment_run(modeldb_config)\n",
    "\n",
    "# Get callbacks and save paths\n",
    "model_data_path_prefix = os.path.join(model_data_path_prefix, modeldb_expt_run.name)\n",
    "callbacks = modeldb_tf_utilities.get_tf_callbacks(modeldb_expt_run, model_data_path_prefix)\n",
    "\n",
    "# Save some attributes before training starts\n",
    "log_model_attributes(modeldb_expt_run)\n",
    "\n",
    "# Define model \n",
    "#loss=tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss=tf.keras.losses.MeanSquaredError()\n",
    "optimizer=tf.optimizers.Adam(learning_rate=initial_lr)\n",
    "sort_model = get_sort_model(numeric_preprocessor=feature_normalizer)\n",
    "#sort_model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "sort_model.compile(loss=loss, optimizer=optimizer)# Log the model\n",
    "log_model_summary(modeldb_expt_run, sort_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 31s 31s/step - loss: 20.1254 - val_loss: 4.2235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.22352, saving model to gs://personalization-tensorflow/models/text_features/text_features_4layer_1024_target_0_458_regression_bhcb1jke/checkpoints/\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 14.2481 - val_loss: 7.5655\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 4.22352\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.9282 - val_loss: 16.9978\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.22352\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.9946 - val_loss: 17.1637\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.22352\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 11.8844 - val_loss: 13.2634\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.22352\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.7767 - val_loss: 9.8701\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 4.22352\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.2933 - val_loss: 7.8505\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 4.22352\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.2750 - val_loss: 6.8462\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 4.22352\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.3690 - val_loss: 6.5459\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 4.22352\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 10.5419 - val_loss: 6.7394\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 4.22352\n",
      "INFO:tensorflow:Assets written to: gs://personalization-tensorflow/models/text_features/text_features_4layer_1024_target_0_458_regression_bhcb1jke/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 211015 05:51:48 builder_impl:774] Assets written to: gs://personalization-tensorflow/models/text_features/text_features_4layer_1024_target_0_458_regression_bhcb1jke/saved_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step - loss: 6.7147\n"
     ]
    }
   ],
   "source": [
    "# fairing:include-cell\n",
    "# Start the training process\n",
    "try:\n",
    "    start_time = time.time()  \n",
    "    cached_ds = training_data.cache()\n",
    "    cached_validation_ds = validation_data.cache()\n",
    "    # Fit the model\n",
    "    model_history = sort_model.fit(cached_ds, validation_data=cached_validation_ds, epochs=num_epochs, callbacks=callbacks)\n",
    "    # Log time taken to fit model\n",
    "    modeldb_expt_run.log_metric('model_fit_run_duration_in_secs', (time.time() - start_time))            \n",
    "    # Save Model\n",
    "    model_save_path = os.path.join(model_data_path_prefix, 'saved_model/')\n",
    "    sort_model.save(model_save_path)\n",
    "    # Log other metrics from model including validation data performance\n",
    "    log_model_metrics(modeldb_expt_run, sort_model, model_save_path, validation_data)\n",
    "    modeldb_expt_run.log_tag('success')\n",
    "except:\n",
    "    modeldb_expt_run.log_tag('failed_run')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate Performance of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for generating labels is 15.988433837890625secs\n"
     ]
    }
   ],
   "source": [
    "# fairing:include-cell\n",
    "st = time.time()\n",
    "\n",
    "# Get predicted labels for validation data\n",
    "pred_indices_raw = sort_model.predict(validation_data)\n",
    "pred_indices = (pred_indices_raw > 0.5)\n",
    "\n",
    "# Get true labels for test data\n",
    "iterator = test_data.as_numpy_iterator()\n",
    "true_labels = np.array([])\n",
    "for x in iterator:\n",
    "    true_labels = np.append(true_labels, x[-1])\n",
    "print(\"Time taken for generating labels is {}secs\".format(time.time() - st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 6, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-734167fb0fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Print Some Performance Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mcr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclass_0_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1989\u001b[0m             )\n\u001b[1;32m   1990\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1992\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of classes, 6, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "# fairing:include-cell\n",
    "# Making sure directory is removed if already exists\n",
    "if os.path.exists('/tmp/plots'):        \n",
    "    shutil.rmtree('/tmp/plots')\n",
    "os.mkdir('/tmp/plots')\n",
    "\n",
    "target_names = ['class 0', 'class 1']\n",
    "\n",
    "# Print Some Performance Metrics\n",
    "print(classification_report(true_labels, pred_indices, target_names = target_names, zero_division = 0))\n",
    "cr = classification_report(true_labels, pred_indices, target_names = target_names, zero_division=0, output_dict = True)\n",
    "class_0_recall = np.around(cr['class 0']['recall'], decimals=5)\n",
    "class_1_recall = np.around(cr['class 1']['recall'], decimals=5)\n",
    "modeldb_expt_run.log_metrics({'Recall_Class_0': class_0_recall, 'Recall_Class_1': class_1_recall, })\n",
    "\n",
    "\n",
    "# Create ROC curve\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, pred_indices_raw)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve')\n",
    "plt.ylabel('TPR')\n",
    "plt.xlabel('FPR')\n",
    "plt.savefig('/tmp/plots/roc.png')\n",
    "modeldb_expt_run.log_artifact('ROC', '/tmp/plots/roc.png')\n",
    "\n",
    "# Create PR curve\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "precision, recall, thresholds = precision_recall_curve(true_labels, pred_indices_raw)\n",
    "plt.plot(recall, precision)\n",
    "plt.title('PR curve')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.savefig('/tmp/plots/pr.png')\n",
    "modeldb_expt_run.log_artifact('PR', '/tmp/plots/pr.png')\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(true_labels, pred_indices)\n",
    "cm_df = pd.DataFrame(cm, index = target_names, columns = target_names)\n",
    "cm_normalize_df = pd.DataFrame(normalize(cm, 'l1', axis = 1), index = target_names, columns = target_names)\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "svm = sns.heatmap(cm_df, annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "figure = svm.get_figure()    \n",
    "figure.savefig('/tmp/plots/cm.png')\n",
    "\n",
    "figure = plt.figure(figsize=(5, 5))\n",
    "svm = sns.heatmap(cm_normalize_df, annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "figure = svm.get_figure()    \n",
    "figure.savefig('/tmp/plots/cm_norm.png')\n",
    "\n",
    "# Saving confusion_matrix\n",
    "modeldb_expt_run.log_artifact('confusion_matrix', '/tmp/plots/cm.png')\n",
    "modeldb_expt_run.log_artifact('confusion_matrix_normalized', '/tmp/plots/cm_norm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "# Get LTR Metrics on a test-data set\n",
    "if os.path.exists('/tmp/data'):        \n",
    "    shutil.rmtree('/tmp/data')\n",
    "os.mkdir('/tmp/data')\n",
    "\n",
    "# Get predictions from the model\n",
    "st = time.time()\n",
    "model_predictions = sort_model.predict(test_dataset)\n",
    "print('Process took {}secs'.format(time.time() - st))\n",
    "\n",
    "# Construct a pandas dataframe with scores and target\n",
    "st = time.time()\n",
    "model_results_df = pd.DataFrame()\n",
    "for f, t in test_dataset:\n",
    "    temp = pd.DataFrame()\n",
    "    temp[target_column] = t\n",
    "    temp['customer_id'] = f['customer_id']\n",
    "    model_results_df = model_results_df.append(temp)\n",
    "print('Process took {}secs'.format(time.time() - st))\n",
    "model_results_df['predicted'] = model_predictions\n",
    "prediction_column = 'predicted'\n",
    "\n",
    "# Call the function to evaluate LTR metrics\n",
    "model_metrics = pd.DataFrame()\n",
    "model_hit_rate, model_ndcg = evaluation_utilities.get_ltr_metrics(model_results_df, \n",
    "                                                                         max_rank, \n",
    "                                                                         target_column, \n",
    "                                                                         prediction_column)\n",
    "model_metrics['hit_rate'] = model_hit_rate\n",
    "model_metrics['ndcg'] = model_ndcg\n",
    "\n",
    "modeldb_expt_run.log_metrics({'HR@10': model_hit_rate[9], 'NDCG@10': model_ndcg[9]})\n",
    "model_ndcg.to_csv(f'/tmp/data/{modeldb_expt_run.name}.csv')\n",
    "modeldb_expt_run.log_artifact('LTRMetrics', f'/tmp/data/{modeldb_expt_run.name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fairing:include-cell\n",
    "modeldb_expt_run.log_metric('experiment_run_duration_in_secs', (time.time() - start_time))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run below cells to get information about the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_sort_model = get_sort_model(feature_normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a_sort_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort_model.get_layer(index=-1).summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
