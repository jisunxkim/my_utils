{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd09a5d9-95bb-4137-8537-42b3493e235a",
   "metadata": {},
   "source": [
    "# Create a TF Job from a Jupyter Notebook Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc38f1e-b738-49cc-81bc-e4ea9ac43a89",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration Parameters for the TF Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1a509f99-fbe6-435d-93c6-a6f212bc4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook to be converted to TF Job\n",
    "#NOTE that any cells in the notebook that need to converted to a TFJob NEEDS TO HAVE \"# fairing:include-cell\" included in the beginning of the cell\n",
    "jupyter_notebook_name = 'tf_model_trainer.ipynb'\n",
    "\n",
    "# Additional files used by the notebook\n",
    "# If an additional library not present in the base image is being used, add this to a requirement.txt file and add the requirements.txt file to the\n",
    "# list of files in the input_files\n",
    "additional_files =['../utilities/common_utilities.py', \n",
    "                 '../utilities/evaluation_utilities.py', \n",
    "                 '../utilities/modeldb_tf_utilities.py',\n",
    "                 '../utilities/from_tfrecords.py',\n",
    "                 '../utilities/to_tfrecords.py',\n",
    "                 '../utilities/google_utils.py',\n",
    "                 '../utilities/model_utilities.py',\n",
    "                 '../model_configs/text_based_tfrecord_config.yaml',\n",
    "                 '../model_configs/text_based_config.yaml']\n",
    "\n",
    "\n",
    "# Cluster configuration\n",
    "num_cpu = 6 # Number of CPUs\n",
    "cpu_memory=40  #Memory in Gigs   \n",
    "num_gpu = 0  # Number of GPUs to be allocated\n",
    "run_id = '5'\n",
    "additional_tag = f'nont_{run_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11245b8d-6912-43ac-a7ef-8ee72d474d1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up TF Job in a Kubernetes Pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e49cd-ea11-410f-98e2-dc509dfce2a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "bd404c32-d2b0-4aee-83ae-684dcf43a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubeflow import fairing \n",
    "from kubeflow.fairing.preprocessors.converted_notebook import ConvertNotebookPreprocessorWithFire\n",
    "from kubeflow.fairing.builders import append\n",
    "from kubeflow.fairing.deployers import job\n",
    "from kubeflow.fairing import constants\n",
    "from kubeflow.fairing.builders import cluster\n",
    "import sys\n",
    "sys.path.append('../utilities/')\n",
    "import common_utilities\n",
    "import os\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a389c-30b9-4d97-a856-775519cd7967",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating TFJob Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "547e51f7-2251-458a-a04f-0a31f121e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zulilymodeltraining\n",
      "rmenon\n"
     ]
    }
   ],
   "source": [
    "GCP_PROJECT = fairing.backends.gcp.guess_project_name()\n",
    "NAMESPACE = fairing.backends.utils.get_current_k8s_namespace()\n",
    "print(GCP_PROJECT)\n",
    "print(NAMESPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f61dcefb-e773-40f1-bd3f-d7125aba6dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up google container repositories (GCR) for storing output containers\n",
    "base_name = jupyter_notebook_name.split(\".\")[0]\n",
    "PROJECT_NAME = 'p13n-model-training'\n",
    "IMAGE_NAME = base_name  #The generated TFJob image will start with this name\n",
    "DOCKER_REGISTRY = 'gcr.io/{}/{}/{}'.format(GCP_PROJECT, NAMESPACE, PROJECT_NAME)\n",
    "# Base image to use for creating the TFJob image\n",
    "BASE_IMAGE = f'gcr.io/{GCP_PROJECT}/kubeflow-notebooks/jupyter_tensorflow:v1.3.0_tensorflow-2.5.0-v3'.format(GCP_PROJECT)\n",
    "# Latest image as of June 2021\n",
    "constants.constants.KANIKO_IMAGE = \"gcr.io/kaniko-project/executor:v1.6.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "230576f7-5b72-4415-af56-89646dc6d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 211210 05:32:11 converted_notebook:191] Converting tf_model_trainer.ipynb to tf_model_trainer.py\n",
      "[I 211210 05:32:11 converted_notebook:194] Creating entry point for the class name None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('tf_model_trainer.py'),\n",
       " '../model_configs/text_based_tfrecord_config.yaml',\n",
       " '../utilities/from_tfrecords.py',\n",
       " '../utilities/evaluation_utilities.py',\n",
       " '../utilities/google_utils.py',\n",
       " '../utilities/common_utilities.py',\n",
       " '../utilities/to_tfrecords.py',\n",
       " '../utilities/modeldb_tf_utilities.py',\n",
       " '../utilities/model_utilities.py',\n",
       " '../model_configs/text_based_config.yaml']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts the notebook mentioned in the notebook_file parameter.\n",
    "# All the code in the cells with fairing comments will be added to a python file with the same name as the ipython notebook.\n",
    "# Later on while building the docker image this python file will be added to the docker image. \n",
    "preprocessor = ConvertNotebookPreprocessorWithFire(notebook_file = jupyter_notebook_name)\n",
    "\n",
    "if not preprocessor.input_files:\n",
    "    preprocessor.input_files = set()\n",
    "preprocessor.input_files =  set([os.path.normpath(f) for f in additional_files])\n",
    "preprocessor.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9b993de1-5d5a-4574-8005-e4aa0da9a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 211210 05:32:11 cluster:46] Building image using cluster builder.\n",
      "[I 211210 05:32:11 base:107] Creating docker context: /tmp/fairing_context_9oyn15ga\n",
      "[I 211210 05:32:11 converted_notebook:191] Converting tf_model_trainer.ipynb to tf_model_trainer.py\n",
      "[I 211210 05:32:11 converted_notebook:194] Creating entry point for the class name None\n",
      "[W 211210 05:32:11 gcp:65] Not able to find gcp credentials secret: user-gcp-sa\n",
      "[W 211210 05:32:11 gcp:67] Trying workload identity service account: default-editor\n",
      "[W 211210 05:32:11 manager:298] Waiting for fairing-builder-hcc9n-cbthm to start...\n",
      "[W 211210 05:32:11 manager:298] Waiting for fairing-builder-hcc9n-cbthm to start...\n",
      "[W 211210 05:32:11 manager:298] Waiting for fairing-builder-hcc9n-cbthm to start...\n",
      "[W 211210 05:32:12 manager:298] Waiting for fairing-builder-hcc9n-cbthm to start...\n",
      "[I 211210 05:32:13 manager:303] Pod started running True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E1210 05:32:17.622770       1 aws_credentials.go:77] while getting AWS credentials NoCredentialProviders: no valid providers in chain. Deprecated.\n",
      "\tFor verbose messaging see aws.Config.CredentialsChainVerboseErrors\n",
      "\u001b[36mINFO\u001b[0m[0004] Retrieving image manifest gcr.io/zulilymodeltraining/kubeflow-notebooks/jupyter_tensorflow:v1.3.0_tensorflow-2.5.0-v3\n",
      "\u001b[36mINFO\u001b[0m[0004] Retrieving image gcr.io/zulilymodeltraining/kubeflow-notebooks/jupyter_tensorflow:v1.3.0_tensorflow-2.5.0-v3 from registry gcr.io\n",
      "\u001b[36mINFO\u001b[0m[0004] Retrieving image manifest gcr.io/zulilymodeltraining/kubeflow-notebooks/jupyter_tensorflow:v1.3.0_tensorflow-2.5.0-v3\n",
      "\u001b[36mINFO\u001b[0m[0004] Returning cached image manifest\n",
      "\u001b[36mINFO\u001b[0m[0004] Built cross stage deps: map[]\n",
      "\u001b[36mINFO\u001b[0m[0004] Retrieving image manifest gcr.io/zulilymodeltraining/kubeflow-notebooks/jupyter_tensorflow:v1.3.0_tensorflow-2.5.0-v3\n",
      "\u001b[36mINFO\u001b[0m[0004] Returning cached image manifest\n",
      "\u001b[36mINFO\u001b[0m[0004] Retrieving image manifest gcr.io/zulilymodeltraining/kubeflow-notebooks/jupyter_tensorflow:v1.3.0_tensorflow-2.5.0-v3\n",
      "\u001b[36mINFO\u001b[0m[0004] Returning cached image manifest\n",
      "\u001b[36mINFO\u001b[0m[0004] Executing 0 build triggers\n",
      "\u001b[36mINFO\u001b[0m[0004] Checking for cached layer gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer/cache:e9f72e06a7ad144205609238b7b668ed25bf315bb69be740ec8f62ec61f24f47...\n",
      "\u001b[36mINFO\u001b[0m[0005] Using caching version of cmd: RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0005] Unpacking rootfs as cmd COPY /app/ /app/ requires it.\n",
      "\u001b[36mINFO\u001b[0m[0150] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0150] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0150] Changed working directory to /app/\n",
      "\u001b[36mINFO\u001b[0m[0150] Creating directory /app/\n",
      "\u001b[36mINFO\u001b[0m[0150] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0150] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0150] No files changed in this command, skipping snapshotting.\n",
      "\u001b[36mINFO\u001b[0m[0150] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0150] Found cached layer, extracting to filesystem\n",
      "\u001b[36mINFO\u001b[0m[0150] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0150] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0150] Pushing image to gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:CB9695A6\n",
      "\u001b[36mINFO\u001b[0m[0151] Pushed image to 1 destinations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 211210 05:35:03 cluster:106] Cleaning up job fairing-builder-hcc9n...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "\n",
      "gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:CB9695A6\n",
      "This process took 171.94618797302246secs\n"
     ]
    }
   ],
   "source": [
    "# This builds the base image as a pod within the Kubernetes cluster\n",
    "st = time.time()\n",
    "# We use the automatically generated Dockerfile.\n",
    "# If you have a custom Dockerfile to use uncomment the parameter below\n",
    "cluster_builder = cluster.cluster.ClusterBuilder(registry = DOCKER_REGISTRY,\n",
    "                                                 base_image = BASE_IMAGE,\n",
    "                                                 image_name = IMAGE_NAME,\n",
    "                                                 preprocessor = preprocessor,\n",
    "                                                 #dockerfile_path=\"Dockerfile\",\n",
    "                                                 pod_spec_mutators = [fairing.backends.gcp.add_gcp_credentials_if_exists], #required to have correct serivceAccount specified to run pod with\n",
    "                                                 context_source = cluster.gcs_context.GCSContextSource(gcp_project=GCP_PROJECT),\n",
    "                                                 cleanup=True)\n",
    "cluster_builder.build()\n",
    "cluster_image = cluster_builder.image_tag\n",
    "print(\"----------------------------------------------------------\\n\")\n",
    "print(cluster_image)\n",
    "print(\"This process took {}secs\".format(time.time() - st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5c601a7f-3127-4267-b5ff-bbd3b09cb794",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 211210 05:35:03 converted_notebook:191] Converting tf_model_trainer.ipynb to tf_model_trainer.py\n",
      "[I 211210 05:35:03 converted_notebook:194] Creating entry point for the class name None\n",
      "[W 211210 05:35:03 append:50] Building image using Append builder...\n",
      "[I 211210 05:35:03 base:107] Creating docker context: /tmp/fairing_context_yfc0y71a\n",
      "[I 211210 05:35:03 converted_notebook:191] Converting tf_model_trainer.ipynb to tf_model_trainer.py\n",
      "[I 211210 05:35:03 converted_notebook:194] Creating entry point for the class name None\n",
      "[W 211210 05:35:03 base:88] tf_model_trainer.py already exists in Fairing context, skipping...\n",
      "[I 211210 05:35:03 docker_creds_:234] Loading Docker credentials for repository 'gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:CB9695A6'\n",
      "[I 211210 05:35:03 docker_creds_:152] Invoking 'docker-credential-gcloud' to obtain Docker credentials.\n",
      "[I 211210 05:35:04 docker_creds_:175] Successfully obtained Docker credentials.\n",
      "[W 211210 05:35:04 append:54] Image successfully built in 1.5439672656357288s.\n",
      "[W 211210 05:35:04 append:94] Pushing image gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:88FC693C...\n",
      "[I 211210 05:35:04 docker_creds_:234] Loading Docker credentials for repository 'gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:88FC693C'\n",
      "[I 211210 05:35:04 docker_creds_:152] Invoking 'docker-credential-gcloud' to obtain Docker credentials.\n",
      "[I 211210 05:35:05 docker_creds_:175] Successfully obtained Docker credentials.\n",
      "[W 211210 05:35:06 append:81] Uploading gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:88FC693C\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:e831a8ee95e00ef2356637cc5ce030baf977544618bdc31f11f98334b9d70032 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:9e2a5662571ceb222655bc70469b4fc307288e8b4a4bd4615289debd7e70ac04 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:47a40e5c1ebd166f9e90c4326ce45a563e2d4466affdd4777aab3344559e6e3c exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:193bcc3c84a00960beee00035d5d0612e4e303fe788418f38fab61d44990e6ec exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:5d3b2c2d21bba59850dac063bcbb574fddcb6aefb444ffcc63843355d878d54f exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:89732bc7504122601f40269fc9ddfb70982e633ea9caf641ae45736f2846b004 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:f23f5eb4889089174f76b62717f107980ea3e731d0e1c0ad5006f75caf99795b exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:9bf69d9f2ed5906a831dd9c9eecafc3d237db0d253cb0fce7e80ecbfd5a425f9 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:882ff4684fa3612aa2f196991aa736c0ba380fc6ac45906a1a4a2de2a9a3443e exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:ed0bb674307879a8e60923f64031fcf04d15ea90a5e5cc6d13e3c144153155d0 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:3fc2062ea6672189447be7510fb7d5bc2ef2fda234a04b457d9dda4bba5cc635 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:2b98b93c7b80f0c9231aa19af5c5a0d78ef0746c64ec768d2d5c0e89b065c765 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:8219aaed543176ab7dd15bd4a321cbaa4a77c26ec8d19fef84cf669cea261580 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:331de03f8b34f50f55e3b8728124afaaf83d7c0901135a4bb0fa0df9c6d617e8 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:4a926989e18f3b2c86be2c5639a6dcd1a573f925505151e147f11b44fa72707e exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:92ac630da84f738590ddc4f35a0e19f0f33fadd1f90c564fe32c013ba3ce3dfd exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:003b4aa48f92847612b3a74b58e90cbcc24889baaf0dd23879fb044d09012de1 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:843a82d6dc56931547d41baa88f4b5984fd89f56fc4f964f5d4c31f9d04cc7c9 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:bc6c2fc8a3ae74bc6396179a13f2508e7ab4040d85cee12766b219b0c7152707 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:0de4eb2002fd51c79b8d591b4d3265c3891e0ce101c472f3d8ae466ab58f7c0e exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:75adf526d75b82eb4f9981cce0b23608ebe6ab85c3e1ab2441f29b302d2f9aa8 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:e10490cc928de464c9e57baf489d3d77eefc653ae0ea48dd098d2519d642e8d3 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:ea62e62660ca3911f1b909e151018c7ebe1a8a02b5832ee6deeec1e0f0736622 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:21236c356b97ceded1e4cdd1ffe7a998c1812706ab160933bf1e336a8678dbd7 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:b552a5d4d0dcdbaf0ff54c8329a249ebd22c3cf39999d533456a03d322335cb3 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:c863f9e0085499a4abc74662afd09fb0ec338785729ed4d6cdcafb3307d762e3 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:d660c97c2a5b61e79f04745597d009fa9f453f0b9cf4b32ca0d405628751ff4d exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:be53d12e46f5db5e5cbcfb8a38e3e5bd117c00546a7514deae4093561dcd0513 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:83874d1c325dd55dd3507fe98f583571cf975b5cf26cac1a25a1e2e72165e15e exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:ede16054b2a26186454e8cdc05cbdb6eb39682f93f915a001cefac0da80c8ba2 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:55c612fbf2794c3799b5fa3ca930b11faf3bc3c74b8f004f263c98b8fce6b811 exists, skipping\n",
      "[I 211210 05:35:06 docker_session_:280] Layer sha256:03995ec1b4c69127cefb720a8f0dae61a6a3246d79b0647a4b82ea81d5f1f21d exists, skipping\n",
      "[I 211210 05:35:07 docker_session_:284] Layer sha256:c7d3568f0b865b39d969013bdbfeb3be30a00e813ddc2ac4f9a88059e9c88754 pushed.\n",
      "[I 211210 05:35:07 docker_session_:284] Layer sha256:97731fdb346ef9b6ec21222baa1573a5c29f9078d250408c0233a59040788ac8 pushed.\n",
      "[I 211210 05:35:07 docker_session_:334] Finished upload of: gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:88FC693C\n",
      "[W 211210 05:35:07 append:98] Pushed image gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:88FC693C in 2.467103660106659s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "\n",
      "gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:88FC693C\n",
      "This process took 4.318068265914917secs\n"
     ]
    }
   ],
   "source": [
    "# This step adds the additional files that are required.\n",
    "st = time.time()\n",
    "preprocessor.preprocess()\n",
    "builder = append.append.AppendBuilder(registry=DOCKER_REGISTRY,\n",
    "                                      image_name=IMAGE_NAME,\n",
    "                                      base_image = cluster_image,\n",
    "                                      preprocessor=preprocessor)\n",
    "\n",
    "# If this step fails runs gcloud auth configure-docker in terminal\n",
    "builder.build()\n",
    "cluster_image = builder.image_tag\n",
    "print(\"----------------------------------------------------------\\n\")\n",
    "print(cluster_image)\n",
    "print(\"This process took {}secs\".format(time.time() - st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebdf5e-5a70-4d57-9f61-e5d6407eac9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a TF Job config and launching in Kubernetes pod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893214a3-4d6c-499a-9fa1-e039fa36dc24",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f8ade28d-3bba-421d-ad87-86cdf04ab29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_job_trial_spec(script_name: str, tf_job_image: str, cpu_request: float, memory_request: float, num_gpu: int = 0, additional_tag: str = '') -> dict:\n",
    "    \"\"\"\n",
    "    Create trial spec for tensorflow job. \n",
    "    Read more about this here: https://www.kubeflow.org/docs/components/training/tftraining/\n",
    "    Currently we are creating a tfjob with 1 worker and not doing distributed training. We will provide example of distributed training in future version.\n",
    "    Additional tag is any additional tag you want to provide to the job name. \n",
    "    The name of the TFJob instance created in the Kubernetest pod will be f\"{script_name}_{num_gpu}gpu_{cpu_request}c_{memory_request}gm_{additional_tag}\" \n",
    "    \n",
    "   :param script_name: Name of the jupyter script that was converted to a TFJob (without the ipynb extension). This name will also be used as the prefix for \n",
    "   the name of the TFJob instance that will be created in a Kubernetes pod (see above for how full name of instance is created)\n",
    "   :param tf_job_image: Name of TF Job image to be deployed\n",
    "   :param cpu_request: Number of CPUs requested\n",
    "   :param memory_request: Gigs of memory requested\n",
    "   :param num_gpu: Number of GPUs requested\n",
    "   :param additional_tag: Additional tag added to name of created TFJob instance (see above for how full name of instance is created)\n",
    "   \"\"\"\n",
    "\n",
    "    cpu_limit = cpu_request\n",
    "    memory_limit = memory_request\n",
    "    \n",
    "    resources_dict = {\n",
    "        \"requests\": {\n",
    "            \"cpu\": f\"{cpu_request}\",\n",
    "            \"memory\": f\"{memory_request}Gi\" \n",
    "        },\n",
    "        \"limits\": {\n",
    "            \"cpu\": f\"{cpu_limit}\",\n",
    "            \"memory\": f\"{memory_limit}Gi\" \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if num_gpu > 0:\n",
    "        resources_dict[\"limits\"][\"nvidia.com/gpu\"] = f\"{num_gpu}\"\n",
    "        resources_dict[\"requests\"][\"nvidia.com/gpu\"] = f\"{num_gpu}\" \n",
    "    \n",
    "    if additional_tag:\n",
    "        additional_tag = '-' + additional_tag\n",
    "    \n",
    "    job_name = f\"{script_name}_{num_gpu}gpu_{cpu_request}c_{memory_request}gb{additional_tag}\"\n",
    "    job_name = job_name.replace(\".\", \"-\")\n",
    "    job_name = job_name.replace(\"_\", \"-\")\n",
    "        \n",
    "    trial_spec = {\n",
    "        \"apiVersion\": \"kubeflow.org/v1\",\n",
    "        \"kind\": \"TFJob\",\n",
    "        \"metadata\": {\n",
    "            \"name\": job_name\n",
    "        },\n",
    "        \"spec\": {\n",
    "            # https://www.kubeflow.org/docs/components/training/tftraining/#tensorflow-logs\n",
    "            \"cleanPodPolicy\": \"All\",\n",
    "            \"tfReplicaSpecs\": {\n",
    "                \"Worker\" : {\n",
    "                    \"replicas\": 1,\n",
    "                    \"template\": {\n",
    "                        \"metadata\": {\n",
    "                            \"annotations\": {\n",
    "                                 \"sidecar.istio.io/inject\": \"false\"\n",
    "                            }\n",
    "                        },\n",
    "                    \"spec\":{\n",
    "                        \"serviceAccountName\": \"default-editor\",\n",
    "                        \"containers\": [\n",
    "                            {\n",
    "                                \"name\": \"tensorflow\",\n",
    "                                \"command\": [\"python\", f\"/app/{script_name}.py\"],                                \n",
    "                                \"image\": tf_job_image,\n",
    "                                \"imagePullPolicy\": \"Always\",\n",
    "                                \"resources\": resources_dict,\n",
    "                                \"workingDir\": \"/app/\"\n",
    "                                }\n",
    "                            ],\n",
    "                            \"restartPolicy\": \"OnFailure\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return trial_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f38ba-928f-463b-a2e8-de3a9268f554",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creating a TF Job config and launching in Kubernetes pod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d95dcd4d-555a-4d08-8afa-2f277b2e21fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the spec that will be deployed: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'kubeflow.org/v1',\n",
       " 'kind': 'TFJob',\n",
       " 'metadata': {'name': 'tf-model-trainer-0gpu-6c-40gb-nont-5-uegyfa6c'},\n",
       " 'spec': {'cleanPodPolicy': 'All',\n",
       "  'tfReplicaSpecs': {'Worker': {'replicas': 1,\n",
       "    'template': {'metadata': {'annotations': {'sidecar.istio.io/inject': 'false'}},\n",
       "     'spec': {'serviceAccountName': 'default-editor',\n",
       "      'containers': [{'name': 'tensorflow',\n",
       "        'command': ['python', '/app/tf_model_trainer.py'],\n",
       "        'image': 'gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:88FC693C',\n",
       "        'imagePullPolicy': 'Always',\n",
       "        'resources': {'requests': {'cpu': '6', 'memory': '40Gi'},\n",
       "         'limits': {'cpu': '6', 'memory': '40Gi'}},\n",
       "        'workingDir': '/app/'}],\n",
       "      'restartPolicy': 'OnFailure'}}}}}}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_hash = common_utilities.random_alphanumeric_str() # Additional tag to differentiate between experiment runs\n",
    "additional_tag = f\"{additional_tag}_{rand_hash}\" # Just extra precaution that we are not over-writing containers.\n",
    "\n",
    "trial_spec = tf_job_trial_spec(script_name=base_name, tf_job_image=cluster_image, cpu_request=num_cpu, memory_request=cpu_memory, \n",
    "                               num_gpu=num_gpu, additional_tag=additional_tag)\n",
    "print(\"Here's the spec that will be deployed: \")\n",
    "trial_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0839ed35-64d1-492f-8fa0-98f8c5defa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'kubeflow.org/v1',\n",
       " 'kind': 'TFJob',\n",
       " 'metadata': {'creationTimestamp': '2021-12-10T05:35:07Z',\n",
       "  'generation': 1,\n",
       "  'managedFields': [{'apiVersion': 'kubeflow.org/v1',\n",
       "    'fieldsType': 'FieldsV1',\n",
       "    'fieldsV1': {'f:spec': {'.': {},\n",
       "      'f:tfReplicaSpecs': {'.': {},\n",
       "       'f:Worker': {'.': {},\n",
       "        'f:replicas': {},\n",
       "        'f:template': {'.': {},\n",
       "         'f:metadata': {'.': {},\n",
       "          'f:annotations': {'.': {}, 'f:sidecar.istio.io/inject': {}}},\n",
       "         'f:spec': {'.': {},\n",
       "          'f:containers': {},\n",
       "          'f:restartPolicy': {},\n",
       "          'f:serviceAccountName': {}}}}}}},\n",
       "    'manager': 'Swagger-Codegen',\n",
       "    'operation': 'Update',\n",
       "    'time': '2021-12-10T05:35:07Z'}],\n",
       "  'name': 'tf-model-trainer-0gpu-6c-40gb-nont-5-uegyfa6c',\n",
       "  'namespace': 'rmenon',\n",
       "  'resourceVersion': '329786761',\n",
       "  'selfLink': '/apis/kubeflow.org/v1/namespaces/rmenon/tfjobs/tf-model-trainer-0gpu-6c-40gb-nont-5-uegyfa6c',\n",
       "  'uid': 'c5bfc232-2301-4882-b6a5-a0f64e4e4b74'},\n",
       " 'spec': {'tfReplicaSpecs': {'Worker': {'replicas': 1,\n",
       "    'template': {'metadata': {'annotations': {'sidecar.istio.io/inject': 'false'}},\n",
       "     'spec': {'containers': [{'command': ['python',\n",
       "         '/app/tf_model_trainer.py'],\n",
       "        'image': 'gcr.io/zulilymodeltraining/rmenon/p13n-model-training/tf_model_trainer:88FC693C',\n",
       "        'imagePullPolicy': 'Always',\n",
       "        'name': 'tensorflow',\n",
       "        'resources': {'limits': {'cpu': '6', 'memory': '40Gi'},\n",
       "         'requests': {'cpu': '6', 'memory': '40Gi'}},\n",
       "        'workingDir': '/app/'}],\n",
       "      'restartPolicy': 'OnFailure',\n",
       "      'serviceAccountName': 'default-editor'}}}}}}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kubeflow.tfjob.api import tf_job_client as tf_job_client_module\n",
    "tf_job_client = tf_job_client_module.TFJobClient()\n",
    "tf_job_client.create(trial_spec, namespace=NAMESPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ca086b-7940-465c-af61-1d5802b6e567",
   "metadata": {},
   "source": [
    "## Helpful commands to monitor Kubernetes job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d00176-6e32-4457-a132-cbf1221f6306",
   "metadata": {},
   "source": [
    "Run commands on the terminal\n",
    "* Get list of TF Jobs in cluster: **kubectl get tfjob**\n",
    "* Get list of all pods running TFJobs: **kubectl get po**\n",
    "* Get logs on a particular pod: **kubectl logs -f \\<POD_NAME\\>**\n",
    "    * Find the pod name from the *kubectl get po* command\n",
    "* Delete completed TF Jobs: **kubectl delete TFjob \\<JOB_NAME\\>**\n",
    "    * Find the job name from *kubectl get tfjob* command"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
