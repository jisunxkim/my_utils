{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63532e8-73ee-42a1-97e9-e3d2ee0d49d4",
   "metadata": {},
   "source": [
    "# XGB Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2cdc6-77cc-4780-85bd-bd5a697f6d8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3dbdda6f-1698-450b-8e11-4ac33a0b515c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Load modules\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import multiprocessing\n",
    "import random\n",
    "import time\n",
    "\n",
    "import argparse \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Load custom functions\n",
    "from utils import ds_general as ds\n",
    "from utils.BigQuery import BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e523c17b-3828-4e41-8cc9-e15c53b9123e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_blob_list(config, train_or_test='train', verbose = 1):    \n",
    "    \"\"\"\n",
    "    return list of blobs of train or test data\n",
    "    \"\"\"\n",
    "    if train_or_test.lower() =='train':\n",
    "        blob = config['TRAIN_DATA_BLOB']\n",
    "        max_files = config['MODEL_RUN_PARAMS']['MAX_N_FILES']\n",
    "    elif train_or_test.lower() == 'test':\n",
    "        blob = config['TEST_DATA_BLOB']\n",
    "        max_files = config['MODEL_RUN_PARAMS']['TOTAL_FILES_TO_TEST']\n",
    "    else:\n",
    "        ds.terminate_prog(\"select 'train' or 'test' for data blob\", error = e)\n",
    "        \n",
    "    try: \n",
    "        blobs_list = bq.list_blobs(config['BUCKET'], f\"{blob}\")\n",
    "        blobs_list = [f\"gs://{config['BUCKET']}/\" + b for b in blobs_list]\n",
    "\n",
    "        n_blobs = len(blobs_list) if not max_files \\\n",
    "            else max_files\n",
    "        print(f\"total num of {train_or_test} blobs {len(blobs_list)}\")\n",
    "        print(f\"num of the selected {train_or_test} blobs {n_blobs}\")\n",
    "        return blobs_list[:n_blobs]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ds.terminate_prog(f\"Failed to load {train_or_test}blob list\", error = e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e170ce-7e01-4415-b00b-b0cea23f0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from blob\n",
    "def load_data_from_blob(blob_path):\n",
    "    \"\"\"\n",
    "    Load model data from a blob.\n",
    "    Return a data frame\n",
    "    \"\"\"\n",
    "    temp_df = pd.DataFrame()\n",
    "    result_df = pd.DataFrame()\n",
    "    try:\n",
    "        if type(blob_path) == str:\n",
    "            blob_path = [blob_path]\n",
    "        for i, blob in enumerate(blob_path):\n",
    "            print(f\"{i+1}th blob loading......\")\n",
    "            temp_df = pd.read_parquet(blob)\n",
    "            result_df = result_df.reset_index(drop=True)\n",
    "            result_df = pd.concat([result_df, temp_df], axis=0)\n",
    "        \n",
    "        return result_df\n",
    "            \n",
    "\n",
    "    except Exception as e:\n",
    "        msg = \"Failed to load the blob.\"\n",
    "        print(msg)\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg, error=e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48301cf9-9ccf-445e-a12b-95f9e47d9824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train_data(df, config, get_sample_weight = True):\n",
    "    \"\"\"\n",
    "    Convert raw dataframe to X, y train data.\n",
    "    Sample weight vectors of the length of the data is also returned if set True.\n",
    "    Include data cleaning.\n",
    "    \n",
    "    Return: (X, y, sample_weight_vector)\n",
    "        X: features panda dataframe (n_rows, n_cols)\n",
    "        y: panda series (n_rows, )\n",
    "        sample_weight_vector: numpy array (n_rows, )\n",
    "        \n",
    "    \"\"\"\n",
    "    result = ()\n",
    "    cols_to_drop = []\n",
    "    try:\n",
    "        df = df.dropna(subset=[config['Y_LABEL_COL']], axis = 0)\n",
    "        \n",
    "        for col in config['COLS_TO_DROP']:\n",
    "            if col in df.columns:\n",
    "                cols_to_drop.append(col)\n",
    "    \n",
    "        df = df.drop(cols_to_drop, axis = 1)\n",
    "        y = df[config['Y_LABEL_COL']].astype(int)\n",
    "        X = df.drop(config['Y_LABEL_COL'], axis=1)\n",
    "        \n",
    "        if get_sample_weight:\n",
    "            sample_weight_vector = np.where(\n",
    "                y == config['WEIGHT_LABEL'], \n",
    "                config['POS_SAMPLE_WEIGHT'], 1)\n",
    "            result = (X, y, sample_weight_vector)\n",
    "        else:\n",
    "            result = (X, y, None)\n",
    "        \n",
    "    except Exception as e:\n",
    "        msg = \"Failed to make train data from the dataframe!\"\n",
    "        print(msg)\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg, error = e)\n",
    "\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22e710bd-621e-4953-8360-cfaf7a447438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model_id(config, prefix='model_'):\n",
    "    if prefix:\n",
    "        return prefix + \"_\" + config['MODEL_VERSION'] + str(ds.name_time_id())\n",
    "    else:\n",
    "        return config['MODEL_VERSION'] + str(ds.name_time_id())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b9abd3-147d-46e9-acb4-b2ecefcfc5d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config(f_path, verbose=2):\n",
    "    \"\"\"\n",
    "    Load a config yaml file.\n",
    "    \"\"\"\n",
    "    config = ds.load_config(f_path)\n",
    "    if verbose > 1:\n",
    "        print(config)\n",
    "    elif verbose > 0:\n",
    "        print(f\"config keys: {config.keys()}\")\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ccc5b1-da64-430a-a74f-f0ee9ef7a42d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_data_multi_processing(blobs_list, config, get_sample_weight = True):\n",
    "    \"\"\"\n",
    "    Multi processing of loading model data including data cleaning.\n",
    "    Return: outputs of make_train_data()\n",
    "        \n",
    "    \"\"\"\n",
    "    if not isinstance(blobs_list, list):\n",
    "        blobs_list = [blobs_list]\n",
    "    try: \n",
    "        print(f\"Data loading total {len(blobs_list)} blobs starting {blobs_list[0]}...\")\n",
    "        \n",
    "        splited_loading_list = ds.split_list_n_size(\n",
    "                blobs_list, \n",
    "                config['MODEL_RUN_PARAMS']['N_FILES_EACH_LOADING'])\n",
    "        df_list = []\n",
    "        \n",
    "        p = multiprocessing.Pool(config['MODEL_RUN_PARAMS']['N_CPUS'])\n",
    "        pool_results = p.map(\n",
    "            func=load_data_from_blob, \n",
    "            iterable=splited_loading_list)\n",
    "        p.close()\n",
    "        p.join()\n",
    "        \n",
    "        data_df = merge_df_list(pool_results)\n",
    "\n",
    "        results = make_train_data(\n",
    "            df=data_df,\n",
    "            config=config,\n",
    "            get_sample_weight=True)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        msng = f\"Failed in geting model data by multi processing starting {blobs_list[0]}.\"\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg=msg, error=e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe6fe7b-a73d-4210-8b3f-f33e457f9f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_df_list(df_list):\n",
    "    if type(df_list) != list:\n",
    "        raise Exception(\"ERROR: should be a lsit of df to merge!!!\")\n",
    "    print(f\"merging {len(df_list)} dataframes started.....\")\n",
    "    try:\n",
    "        result_df = pd.DataFrame()\n",
    "        for df in df_list:\n",
    "            result_df = result_df.reset_index(drop=True)\n",
    "            result_df = pd.concat([result_df, df], axis=0)\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        msg = \"failed to merge dataframes\"\n",
    "        print(msg)\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg=msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bb7b6a9-5217-4248-8f56-0e91707db4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load best parameters\n",
    "def load_best_params(bucket_name=None, blob_name=None, \n",
    "                     param_fname = None,\n",
    "                     param_dict = None,\n",
    "                     verbose = 1):\n",
    "    params = None\n",
    "    source = None\n",
    "    try:\n",
    "        if blob_name:\n",
    "            param_fname = './tmp/temp_best_params.pkl'\n",
    "            bq.download_file_from_blob(\n",
    "                bucket_name,\n",
    "                blob_name,\n",
    "                param_fname,\n",
    "                verbose\n",
    "            )\n",
    "            with open(param_fname, 'rb') as file:\n",
    "                params = pickle.load(file)\n",
    "                \n",
    "            source = \"BLOB\"\n",
    "        elif param_fname:\n",
    "            with open(param_fname, 'rb') as file:\n",
    "                params = pickle.load(file)\n",
    "                \n",
    "            source = \"FILE\"\n",
    "        elif param_dict:\n",
    "            params = param_dict\n",
    "            \n",
    "            source = \"CONFIG/MANUAL\"\n",
    "        else:\n",
    "            source = \"NO SOURCE\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        msg = \"Failed to load the best_params\"\n",
    "        print(e)\n",
    "        ds.terminate_prog(msg=msg, error=e)\n",
    "    \n",
    "    print(f\"Hyper Parameters Loaded from {source}.....\")\n",
    "    if verbose > 0:\n",
    "        print(params)\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f67cf94-14e1-4671-a4b6-4006de3acf76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_metrcs_inline(eval_y, y_pred, loss, best_iter):\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(eval_y, y_pred)\n",
    "    balanced_accuracy_value = balanced_accuracy_score(eval_y, y_pred)\n",
    "    f1_value = f1_score(eval_y, y_pred)\n",
    "    recall_val = recall_score(eval_y, y_pred)\n",
    "    roc_auc_val = roc_auc_score(eval_y, y_pred)\n",
    "    \n",
    "    min_loss = min(loss)\n",
    "    last_loss = loss[-1]\n",
    "    epochs = len(loss)\n",
    "\n",
    "    print(\n",
    "        \"| Accu: %.2f%%\" % (accuracy * 100.0), \\\n",
    "        \"| Bal Accu: %.2f%%\" %(balanced_accuracy_value * 100.0), \\\n",
    "        \"| ROC AUC: %.3f\" %(roc_auc_val), \\\n",
    "        \"| Recall Pos: %.3f\" %(recall_val), \\\n",
    "        \"| F1 Pos: %.3f\" %(f1_value), \\\n",
    "        \"| Epochs: %.0i\" %(epochs), \\\n",
    "        \"| Best Iter Num: %.0i\" %(best_iter), \\\n",
    "        \"| Min loss: %.3f\" %(min_loss), \\\n",
    "        \"| Last loss: %.3f\" %(last_loss) \\\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "000290a9-8376-40bd-932d-da7cad24c116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rand_search_cv(config):\n",
    "    # Trainig model set up\n",
    "    \n",
    "    params_grid = load_best_params(\n",
    "        param_dict = config['TUNE_PARAMS'])\n",
    "    \n",
    "\n",
    "    estimator = XGBClassifier(\n",
    "        eval_metric=config['MODEL_RUN_PARAMS']['EVAL_METRIC'], \n",
    "        seed=123)\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "                        estimator=estimator, \n",
    "                        param_distributions=params_grid, \n",
    "                        scoring = config['MODEL_RUN_PARAMS']['CV_MODEL_SCORE'],\n",
    "                        n_iter= config['MODEL_RUN_PARAMS']['RSCV_N_ITERS'], \n",
    "                        cv=config['MODEL_RUN_PARAMS']['RSCV_CV'],\n",
    "                        verbose=config['VERBOSE'],\n",
    "                        random_state=123)\n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b03c353-5595-4ef4-aabd-dfa9b9476a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config_file_name):\n",
    "    logger = ds.get_logger(level='INFO')\n",
    "    \n",
    "    ds.make_folder('tmp')\n",
    "    \n",
    "    temp_model_fname = './tmp/temp_tuning_model_save.model'\n",
    "    \n",
    "    main_time = ds.time_start_end(msg=\"MAIN\")\n",
    "    # load configuration from a yaml file\n",
    "    config = get_config(config_file_name)\n",
    "\n",
    "    \n",
    "    # load list of blobs\n",
    "    # actual train test will be loaded in each batch process\n",
    "    blobs_list = load_blob_list(config, train_or_test='train')\n",
    "    \n",
    "    # load test blobs and data\n",
    "    test_data_time = ds.time_start_end(msg=\"test_data_load\")\n",
    "    test_blobs_list = load_blob_list(config, train_or_test='test')\n",
    "    test_X, test_y, test_sample_weight_vector = get_model_data_multi_processing(\n",
    "            test_blobs_list, config, get_sample_weight=False)\n",
    "    \n",
    "    ds.df_info(test_X, label=\"test data feature dataframe\")\n",
    "    ds.time_start_end(started=test_data_time, msg=\"test_data_load\")\n",
    "    \n",
    "    \n",
    "    #############################\n",
    "    # Train in Bath processing in Serial: \n",
    "    # continuous Training batch by batch\n",
    "    #############################\n",
    "    split_batch_blob_list = ds.split_list_n_size(\n",
    "        blobs_list, \n",
    "        config['MODEL_RUN_PARAMS']['N_FILES_IN_BATCH'])\n",
    "    total_batch = len(split_batch_blob_list)\n",
    "    \n",
    "    # Iterate each batch\n",
    "    for i, blobs_list in enumerate(split_batch_blob_list):\n",
    "        n_batch = i + 1\n",
    "        batch_idx =f\"Training Batch {str(n_batch)}/{str(total_batch)}\"\n",
    "        print(f\"**** Starting Batch Training: {batch_idx} ****\")\n",
    "        # load train data from the blobs \n",
    "        batch_time = ds.time_start_end(\n",
    "            msg=f\"{batch_idx} train data loading from blobs\")\n",
    "            # multi processing - loading from blobs: \n",
    "        X, y, sample_weight_vector = get_model_data_multi_processing(\n",
    "            blobs_list, config, get_sample_weight=True)\n",
    "            \n",
    "        ############\n",
    "        ds.df_info(X, label=f\"{batch_idx} feature dataframe\")\n",
    "        ds.time_start_end(started=batch_time, msg=f\"{batch_idx} train data loading from blobs\")\n",
    "    \n",
    "        # train on the data\n",
    "        batch_train_time = ds.time_start_end(msg=f\"{batch_idx} Tuning\")\n",
    "        eval_set = ([(X, y), (test_X, test_y)])\n",
    "        try: \n",
    "            \n",
    "            tuning_search = get_rand_search_cv(config)\n",
    "            \n",
    "            tuning_search.fit(\n",
    "                X, y,\n",
    "                eval_set = ([(X, y), (test_X, test_y)]),\n",
    "                verbose=config['EVAL_VERBOSE'])\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            msg = \"Failed in tuning\"\n",
    "            print(msg, e)\n",
    "            ds.terminate_prog(msg=msg, error=e)\n",
    "            \n",
    "        ds.time_start_end(started=batch_train_time, msg=f\"{batch_idx} Tuning\")\n",
    "        ds.time_start_end(started=batch_time, msg=f\"{batch_idx}\")\n",
    "    \n",
    "    logger.info(\"Hyper-parameters tunning DONE\")\n",
    "    logger.info(\"Best parameters:\" + str(tuning_search.best_params_))\n",
    "    \n",
    "    ds.time_start_end(started=main_time, msg=\"MAIN\")\n",
    "    \n",
    "    \n",
    "    ########## results\n",
    "    best_model = tuning_search.best_estimator_\n",
    "    best_params = tuning_search.best_params_\n",
    "    \n",
    "    # Get the evaluation results of the batch\n",
    "    eval_results = best_model.evals_result()\n",
    "    loss_values = eval_results['validation_1']['logloss']  # Change 'rmse' to the appropriate metric used during training\n",
    "    best_iter = best_model.best_iteration\n",
    "    y_pred = best_model.predict(test_X)\n",
    "    print_metrcs_inline(test_y, y_pred, loss_values, best_iter)\n",
    "    \n",
    "    # Save best_paramters and upload to blob\n",
    "    model_id = make_model_id(config, prefix=config[\"MODEL_ID_PREFIX\"])    \n",
    "    \n",
    "    params_file_name = 'best_params_' + model_id +'.pkl'\n",
    "    blob_name = config['ARTIFACT_BLOB'] + '/' + params_file_name\n",
    "    \n",
    "    with open(params_file_name, 'wb') as file:\n",
    "        pickle.dump(best_params, file)\n",
    "    \n",
    "    if config['UPLOAD_MODEL']:\n",
    "        print(f\"Uploading the best parameters, {params_file_name} to GCS\") \n",
    "        bq.upload_file_to_blob(params_file_name, config['BUCKET'], blob_name)\n",
    "        print(f\"uploaded the best parameters to {blob_name}\")\n",
    "    else:\n",
    "        print(\"best parameters NOT uploaded to BLOB. Set True in config to upload!\")\n",
    "    \n",
    "    # Save best_model and upload to blob\n",
    "    model_file_name = 'best_model_' + model_id +'.pkl'\n",
    "    blob_name = config['ARTIFACT_BLOB'] + '/' + model_file_name\n",
    "    \n",
    "    with open(model_file_name, 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "    \n",
    "    if config['UPLOAD_MODEL']:\n",
    "        print(f\"Uploading the best model, {model_file_name} to GCS\") \n",
    "        bq.upload_file_to_blob(model_file_name, config['BUCKET'], blob_name)\n",
    "        print(f\"uploaded the best model to {blob_name}\")\n",
    "    else:\n",
    "        print(\"best model NOT uploaded to BLOB. Set True in config to upload!\")\n",
    "    \n",
    "              \n",
    "    ## Remove the temp model file\n",
    "    if config['REMOVE_LOCAL_TEMP_MODEL']:\n",
    "        os.remove(params_file_name)\n",
    "        print(\"Removed local best parameter file\")\n",
    "        os.remove(model_file_name)\n",
    "        print(\"Removed local best model file\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e24b4-37e5-4f9d-b389-1414d5f14209",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"config_file_name\", help=\"config file name\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    bq = BigQuery()\n",
    "    \n",
    "    main(args.config_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b75d86f-3d46-42bf-9608-76a1fd953373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
